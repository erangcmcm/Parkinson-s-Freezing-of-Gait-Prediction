{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U kaggle\n",
    "!mkdir -p ~/.kaggle\n",
    "!cp /home/ec2-user/efs/Various/kaggle.json ~/.kaggle/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ~/.aws\n",
    "!cp /home/ec2-user/efs/Various/credentials ~/.aws/credentials \n",
    "!cp /home/ec2-user/efs/Various/config ~/.aws/config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "packages = ('pytorch_lightning timm==0.6.12 ipywidgets==7.7.1 opencv-python zstandard awscli ' \n",
    "                    +  'transformers librosa torchlibrosa torchaudio torchvision ' \n",
    "                    + 'lion-pytorch segmentation-models-pytorch==0.3.2 ' \n",
    "                    'lightgbm ') # fcwt fsspec[s3] albumentations==1.3.0 \n",
    "!pip install -q -U $packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/data/'\n",
    "TAG = 'tlvmc-parkinsons-freezing-gait-prediction'\n",
    "BUCKET = 'projects-v'\n",
    "region = !cat ~/.aws/config | grep region | awk '{print $3}' \n",
    "DATA_BUCKET = 'projects-e1' if region[0] == 'us-east-1' else BUCKET if region[0] == 'us-east-2' else '' \n",
    "PREFIX = 'walk/'\n",
    "OFFLINE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import boto3\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "import os\n",
    "import io\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "import zstandard as zstd\n",
    "zd = zstd.ZstdDecompressor()\n",
    "zc = zstd.ZstdCompressor()\n",
    "\n",
    "import math\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import cv2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kaggle competitions download -c $TAG -p $DATA_PATH\n",
    "# !unzip -l $DATA_PATH$TAG\".zip\"\n",
    "\n",
    "# # show compressed size of each file in zip and ratio\n",
    "# with zipfile.ZipFile(DATA_PATH + TAG + '.zip', 'r') as f:\n",
    "#     for info in f.infolist():\n",
    "#         print(info.filename, info.compress_size, info.file_size, info.compress_size / info.file_size)\n",
    "#     files = f.namelist()\n",
    "\n",
    "# def uploadFileFromZip(f,):\n",
    "#     s3 = boto3.client('s3')\n",
    "#     zc = zstd.ZstdCompressor()\n",
    "#     with zipfile.ZipFile(DATA_PATH + TAG + '.zip', 'r') as z:\n",
    "#         return s3.put_object(Bucket = BUCKET, \n",
    "#                     Key = PREFIX + 'data/' + f + '.zstd', \n",
    "#                     Body = zc.compress(z.read(f)))\n",
    "\n",
    "# r = Parallel(os.cpu_count())(delayed(uploadFileFromZip)(f) for f in files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3 list with paginator\n",
    "paginator = s3.get_paginator('list_objects_v2')\n",
    "pages = paginator.paginate(Bucket = DATA_BUCKET, Prefix = PREFIX + 'data/')\n",
    "objs = []\n",
    "for page in pages:\n",
    "    for obj in page['Contents']:\n",
    "        objs.append(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum([e['Size'] for e in objs if 'un' in e['Key']])/1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # s3 list with paginator\n",
    "# paginator = s3.get_paginator('list_objects_v2')\n",
    "# pages = paginator.paginate(Bucket = DATA_BUCKET, Prefix = PREFIX + 'cache/')\n",
    "# objs = []\n",
    "# for page in pages:\n",
    "#     for obj in page['Contents']:\n",
    "#         objs.append(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted([e['Key'] for e in objs])[::100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFile(file, save = True):\n",
    "    s3 = boto3.client('s3')\n",
    "    zd = zstd.ZstdDecompressor()\n",
    "    file_path = DATA_PATH + 'data/' + file + '.zstd'\n",
    "    if not os.path.exists(file_path) or os.path.getsize(file_path) == 0:\n",
    "        if save: os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "        fin = s3.get_object(Bucket = DATA_BUCKET, Key = PREFIX + 'data/' + file + '.zstd')['Body']#.read()        \n",
    "        if save:\n",
    "            with open(file_path, 'wb') as f: f.write(fin.read())\n",
    "    out = io.BytesIO()\n",
    "    if os.path.exists(file_path) and os.path.getsize(file_path) > 0:\n",
    "        with open(file_path, 'rb') as fin:\n",
    "            zd.copy_stream(fin, out)\n",
    "    else:        \n",
    "        out.write(fin)\n",
    "    out.seek(0)\n",
    "    # print(out.getbuffer().nbytes)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (7, 4)\n",
    "\n",
    "from IPython.display import display\n",
    "np.random.seed(datetime.datetime.now().microsecond)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = sorted([o['Key'].split('walk/data/')[-1].split('.zstd')[0] for o in objs])\n",
    "display(files[:10])\n",
    "\n",
    "events = pd.read_csv(getFile('events.csv'))\n",
    "subjects = pd.read_csv(getFile('subjects.csv'))\n",
    "tasks = pd.read_csv(getFile('tasks.csv'))\n",
    "daily_metadata = pd.read_csv(getFile('daily_metadata.csv'))\n",
    "defog_metadata = pd.read_csv(getFile('defog_metadata.csv'))\n",
    "tdcsfog_metadata = pd.read_csv(getFile('tdcsfog_metadata.csv'))\n",
    "sample = pd.read_csv(getFile('sample_submission.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUEUE_NAME = 'walk'\n",
    "sqs = boto3.client('sqs')\n",
    "sqs.create_queue(QueueName = QUEUE_NAME)\n",
    "queue_url = sqs.get_queue_url(QueueName = QUEUE_NAME)['QueueUrl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 25k w/0.03wd, 50k w/0.1 wd\n",
    "all_params = [\n",
    "    # [{}] * 1,\n",
    "    # [{'melspec': True, 'n_mels': 16, 'mel_pwr': 0.3}],\n",
    "    # [{'melspec': True, 'n_mels': 16, 'mel_pwr': None}], \n",
    "\n",
    "    # [{'seg': True, 'xformer_layers': 0, 'encoder': 'tu-mobilevitv2_100', \n",
    "    #     'melspec': True, 'n_mels': 16, }],\n",
    "    \n",
    "    [{'xformer_layers': l, 'xformer_init_scale': 0.7, 'rel_pos': e}\n",
    "            for e in ['mlp', None, ]#'bias']\n",
    "            for l in [ 3, 4, 5,]],\n",
    "    [{'xformer_layers': l, 'deberta': True,\n",
    "        'xformer_init_1': 1., 'xformer_init_2': 1,\n",
    "       'xformer_init_scale': 0.7,}\n",
    "            for l in [3, 4, 5]],\n",
    "]\n",
    "def flatten(l): return [item for sublist in l for item in sublist]\n",
    "all_params = flatten(all_params)\n",
    "len(all_params), all_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_params = flatten([all_params] * 2)\n",
    "all_params = [p.copy() for p in all_params]\n",
    "# all_params = random.sample(all_params, 8)\n",
    "for i, p in enumerate(all_params):\n",
    "    # p = p.copy()\n",
    "    p.update( {#'steps': random.randint(25000, 35000),\n",
    "               'step_mult': random.choice([2, 3,]),\n",
    "               'batch_size': random.choice([12, 16, 20, 24,]),\n",
    "\n",
    "                'seq': random.choice([192, 224, 256, 288, 320, 384, ]),\n",
    "                'patch': random.choice([8, 9, 10, 11, 12, 13 ]),\n",
    "               \n",
    "                'alibi': random.choice([0, 2, 4, 8]),\n",
    "                'lion': random.choice([True, False, False]),\n",
    "                'lr': random.choice([0.3e-4, 0.5e-4, 0.7e-4, 1e-4, 2e-4, 3e-4, 5e-4,]),\n",
    "                'weight_decay': round(0.05 * np.exp(np.random.normal(0, 1)), 3),\n",
    "                'frac_pwr_mult': round(np.exp(np.random.normal(0.7, 0.1)), 2),\n",
    "                'frac_rand': round(random.random(), 2), \n",
    "                'stretch_rate': random.choice([0.3, 0.5, 0.7,]),\n",
    "                'dims': random.choice([256, ]),\n",
    "                'act_layer': random.choice(['GELU', 'GELU', 'GELU', 'PReLU', 'CELU']),\n",
    "                'dropout': random.choice([0.1, 0.15, 0.2, 0.25, 0.3]),\n",
    "                'focal_alpha': random.choice([0.1, 0.25, 0.25]),\n",
    "                'focal_gamma': random.choice([1.5, 1.5, 2., 2.5 ]),\n",
    "                'patch_act': random.choice(['Identity', 'Identity', 'Identity', 'Identity',                                             \n",
    "                                            'PReLU', 'PReLU', 'PReLU',\n",
    "                                            'GELU', 'GELU', 'GELU', 'CELU', \n",
    "                                            'Tanh', 'Tanh', \n",
    "                                            'LeakyReLU', 'LeakyReLU', 'LeakyReLU',\n",
    "                                              ]),\n",
    "                'rnn': random.choice([None, ] + ['GRU'] * 5 +['LSTM'] + ['GRU']),\n",
    "                'se_dims': random.choice([0, 8, 16]),\n",
    "                'frac_se': False, #random.choice([True, False]),\n",
    "                'len_se': False, # random.choice([False,]),\n",
    "                'm_se': True, # random.choice([True, ]),\n",
    "                'se_dropout': random.choice([0.2, 0.25, 0.3, ]),\n",
    "                'se_pact': random.choice([0., ]),\n",
    "                # encodes only defog vs tdcsfog\n",
    "\n",
    "\n",
    "                'fast_mult': random.choice([1, 1, 1, 1, 0.5, 0.3, ]),\n",
    "                'final_mult': random.choice([2, 4, 4, 6 ]),\n",
    "                'pre_norm': random.choice([True, False]),\n",
    "\n",
    "                '0x2d57c2': random.choice(['22', '21', '12', ]),\n",
    "                '0xe86b6e': random.choice(['12', '12', '11']),\n",
    "                'fix_final': random.choice([True, True, False, ]),\n",
    "                'mae_divisor': random.choice([1, 2, 5, 10, ]),\n",
    "\n",
    "                'aux_wt': random.choice([0.]),\n",
    "                'v_wt': random.choice([0.03, ]),                \n",
    "                'min_wt': random.choice([3e-3, 0.01,]),\n",
    "                \n",
    "                'frac_adj': random.choice([True, False]),\n",
    "                'm_adj': random.choice([True, True, True, False]),\n",
    "                \n",
    "                'adj_gn': random.choice([0.3, 0.5,]),\n",
    "                'm_adj_gn': random.choice([0.1, 0.2]),\n",
    "\n",
    "                'len_adj': random.choice([True, False, ]),                \n",
    "\n",
    "                'folds': random.choice(['A', 'B',]),# 'C', 'D'])\n",
    "                'patch_dropout': random.choice([0, 0, 0.,  \n",
    "                                                0.05, 0.1, 0.15, 0.2, 0.25]),\n",
    "                # 'frac_gn': random.choice([0., 0.03, 0.1]), \n",
    "\n",
    "                'expanded': random.choice([False, False, False]),\n",
    "            })\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in all_params:\n",
    "    if p['pre_norm']:\n",
    "        p['xformer_init_1'] = 1.\n",
    "        p['xformer_init_2'] = 1.\n",
    "        p['xformer_init_scale'] = 0.7\n",
    "        \n",
    "    # if random.random() < 1/10 and p['rnn'] is None:\n",
    "    #     p['xformer_layers'] = 0\n",
    "    \n",
    "    if random.random() < 1:\n",
    "        p['dims'] = 384\n",
    "        p['nheads'] = 12\n",
    "        p['final_mult'] = 4\n",
    "\n",
    "    if random.random() < 1:\n",
    "        p['xformer_attn_drop_rate'] = 0.\n",
    "        p['xformer_drop_path_rate'] = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RELABEL = True\n",
    "if RELABEL:\n",
    "    for p in all_params:\n",
    "        p['relabel'] = True\n",
    "        p['batch_size'] = 12\n",
    "        p['steps'] = random.choice([40000, 30000 ])\n",
    "        p['focal_alpha'] = 0.25\n",
    "        p['focal_gamma'] = 1.\n",
    "        p['neg_mult'] = random.choice([0.01, 0.03, 0.1, ])\n",
    "        if p['seq'] == 192:\n",
    "            p['seq'] *= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "# random.seed(datetime.datetime.now().microsecond)\n",
    "# random.shuffle(all_params)\n",
    "# # sqs.purge_queue(QueueUrl = queue_url)\n",
    "# N_FOLDS = 4\n",
    "# MAX = 2\n",
    "# base_seed = random.randint(0, 100000)\n",
    "# for i, p in enumerate(all_params[::-1][:MAX]):\n",
    "#     for s in range(1):\n",
    "#         for fold in range(N_FOLDS):\n",
    "#             p['n_folds'] = N_FOLDS\n",
    "#             p['fold'] = fold\n",
    "#             p['seed'] = base_seed + i \n",
    "#             sqs.send_message(QueueUrl = queue_url, MessageBody = json.dumps(p))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # s3 delete all objects beggiing with a certain prefix\n",
    "# s3r = boto3.resource('s3')\n",
    "# bucket = s3r.Bucket(BUCKET)\n",
    "# bucket.objects.filter(Prefix = PREFIX + 'spreds/').delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3.list_objects(Bucket = BUCKET, Prefix = PREFIX + 'spreds/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# sqs.delete_message(QueueUrl = queue_url, ReceiptHandle = rh)\n",
    "# sqs.send_message(QueueUrl = queue_url, MessageBody = json.dumps(eparams))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eparams = random.choice(all_params)\n",
    "# eparams['patch']\n",
    "# eparams['seed'] = random.randint(0, 100000)\n",
    "# eparams['n_folds'] = 4\n",
    "# eparams['fold'] = 0#random.randint(0, 2)\n",
    "# eparams['folds'] = 'A'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    msg = sqs.receive_message(QueueUrl = queue_url, WaitTimeSeconds = 3,\n",
    "                                VisibilityTimeout = 90 * 60,                            \n",
    "                              )['Messages'][0]\n",
    "    rh, body = [msg[e] for e in ['ReceiptHandle', 'Body']]\n",
    "    eparams = json.loads(body)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = eparams.copy()\n",
    "display(params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = results[0]['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "FOLD, SEED = params['fold'], params['seed']\n",
    "N_FOLDS = params.get('n_folds', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_metadata(defog_metadata, tdcsfog_metadata, daily_metadata, subjects, full = True,\n",
    "                    expanded = True):\n",
    "    m1 = defog_metadata.copy()\n",
    "    m1.insert(3, 'Test', 0)\n",
    "    m1 = m1.merge(subjects, on = ['Subject', 'Visit', ], how = 'inner')\n",
    "    assert len(m1) == len(defog_metadata)\n",
    "\n",
    "    m2 = tdcsfog_metadata.copy()\n",
    "    m2 = m2.merge(subjects.drop(columns = 'Visit'), on = ['Subject', ], how = 'inner')\n",
    "    assert len(m2) == len(tdcsfog_metadata)\n",
    "\n",
    "    m3 = daily_metadata.copy()\n",
    "    m3 = m3.merge(subjects, on = ['Subject', 'Visit' ], how = 'inner')\n",
    "    m3.insert(3, 'Test', 0)\n",
    "    m3.insert(4, 'Medication', (defog_metadata.Medication == 'on').mean())\n",
    "    m3.drop(columns = [c for c in m3.columns if 'recording' in c], inplace = True)\n",
    "    assert len(m3) == len(daily_metadata)\n",
    "\n",
    "    metadata = pd.concat([m1, m2, #m3\n",
    "                            ], axis = 0)\n",
    "    metadata.Medication = 1 * (metadata.Medication == 'on')\n",
    "    metadata.Sex = 1 * (metadata.Sex == 'M')\n",
    "    \n",
    "    if expanded:\n",
    "        metadata['num_tests'] = metadata.groupby('Subject').transform(lambda x: x.nunique()).Test\n",
    "        metadata['max_visit'] = metadata.groupby('Subject').transform(lambda x: x.max()).Visit\n",
    "        metadata['visit_medications'] = metadata.groupby(['Subject', 'Visit']).transform('nunique').Medication \n",
    "        metadata['UPDRS_On_vs_Off'] = metadata.UPDRSIII_On - metadata.UPDRSIII_Off\n",
    "        # add 4 columns to dmetadata\n",
    "    \n",
    "\n",
    "    if full:\n",
    "        # null fix\n",
    "        metadata['Uon_null'] = 1 * (metadata.UPDRSIII_On.isnull())\n",
    "        metadata['Uoff_null'] = 1 * (metadata.UPDRSIII_Off.isnull())\n",
    "        metadata.UPDRSIII_On = metadata.UPDRSIII_On.fillna(metadata.UPDRSIII_On.mean())\n",
    "        metadata.UPDRSIII_Off = metadata.UPDRSIII_Off.fillna(metadata.UPDRSIII_Off.mean())\n",
    "        if expanded:\n",
    "            metadata.UPDRS_On_vs_Off = metadata.UPDRS_On_vs_Off.fillna(metadata.UPDRS_On_vs_Off.mean())   \n",
    "\n",
    "    metadata.set_index('Id', inplace = True)\n",
    "    \n",
    "    if full:\n",
    "        metadata['Test_Nonzero'] = 1. * (metadata.Test > 0)\n",
    "        # for i in range(1):\n",
    "        #     metadata['Test{}'.format(i)] = metadata.Test == i\n",
    "        metadata.iloc[:, 1:] = metadata.iloc[:, 1:].astype(np.float32)\n",
    "        metadata.iloc[:, 1:] = (metadata.iloc[:, 1:] - metadata.iloc[:, 1:].mean(0)) / metadata.iloc[:, 1:].std(0)  \n",
    "        metadata.iloc[:, 1:] = metadata.iloc[:, 1:].clip(-3, 3)\n",
    "\n",
    "    msubject = metadata.Subject\n",
    "    metadata.drop(columns = 'Subject', inplace = True)\n",
    "    if full:\n",
    "        metadata = metadata.astype(np.float32)\n",
    "    \n",
    "    m3 = m3.set_index('Id')\n",
    "    assert m3.shape[1] <= metadata.shape[1]; i = 0\n",
    "    while m3.shape[1] < metadata.shape[1]:\n",
    "        m3.insert(m3.shape[1], 'dummy_{}'.format(i), 0); i += 1\n",
    "    m3.iloc[:, -1] = metadata.iloc[:, -1].min() # yes, hack, for default_metadata in dataset.py \n",
    "\n",
    "    return metadata, msubject, m3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata, msubject, dmetadata = prep_metadata(defog_metadata, tdcsfog_metadata, daily_metadata, subjects,\n",
    "                                   expanded = params['expanded'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert metadata.shape[1] == dmetadata.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downsample = {k[2:]: v for k, v in params.items() if k.startswith('0x')}\n",
    "display(downsample)\n",
    "\n",
    "drop_ids = []; id_frac = {}\n",
    "for k, v in downsample.items():\n",
    "    print(k, v)\n",
    "    df = metadata[msubject.loc[metadata.index] == k]\n",
    "    ids = df.sample(frac = 1 - 1 / int(v[0]), random_state = SEED).index.tolist()\n",
    "    drop_ids.extend(ids); print(ids)\n",
    "\n",
    "    ids = {i: 1 / int(v[1]) for i in df.index}\n",
    "    id_frac.update(ids); print(ids)\n",
    "    print()\n",
    "# common_\n",
    "# COMMON = ['2d57c2', 'e86b6e'][:params['sans']]; print(COMMON)\n",
    "# common_events = msubject.reindex(events.Id).isin(COMMON ).values \n",
    "# common_events.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test_downsample = {k[2:]: v for k, v in params.items() if k.startswith('0x')}\n",
    "# # display(downsample)\n",
    "\n",
    "# test_id_frac = {}\n",
    "# for k, v in {'2d57c2': '18', 'e86b6e': '12'}.items():\n",
    "#     print(k, v)\n",
    "#     df = metadata[msubject.loc[metadata.index] == k]\n",
    "#     ids = {i: 1 / int(v[1]) for i in df.index}\n",
    "#     test_id_frac.update(ids); print(ids)\n",
    "#     print()\n",
    "# # test_id_frac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_events = events.Id.isin(drop_ids); print(drop_events.sum())\n",
    "event_frac = events.Id.map(id_frac).fillna(1); print((event_frac < 1).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if params['folds'] in ['A', 'D']:\n",
    "    # pd cut -- size\n",
    "    events['Length'] = (events.Completion - events.Init)\n",
    "    random.seed(SEED)\n",
    "    events['Q'] = events.groupby('Type')['Length'].transform( \n",
    "        lambda x: pd.cut(x, (10 if params['folds'] == 'A'\n",
    "                                else random.randint(5, 15)\n",
    "                             ) ** np.arange(0, 10), labels = False)).fillna(0)\n",
    "    assert events.Q.isnull().sum() == 0\n",
    "    random.seed(datetime.datetime.now().microsecond)\n",
    "    display(events.groupby(['Type', 'Q']).Length.agg(['mean', 'count', 'sum']))\n",
    "\n",
    "elif params['folds'] in ['B', 'C']:\n",
    "    # pd cut -- binary\n",
    "    events['Length'] = (events.Completion - events.Init) #* (~drop_events) * event_frac\n",
    "    random.seed(SEED)\n",
    "    events['Q'] = events['Length'].transform( \n",
    "        lambda x: pd.cut(x, [-1, 5 + 10 * random.random(), 1000000], labels = False) )#.fillna(0)\n",
    "    assert events.Q.isnull().sum() == 0\n",
    "    random.seed(datetime.datetime.now().microsecond)\n",
    "    display(events.groupby(['Type', 'Q']).Length.agg(['mean', 'count', 'sum']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev = sorted(events.Type.dropna().unique())[::]; print(ev)\n",
    "f = ~events.Type.isnull() & (events.Type != 'Turn') & (events.Length > 0.5) & ~drop_events\n",
    "ef = events[f].set_index('Id')\n",
    "etables = []\n",
    "s = ef.Type.map(dict(zip(ev, range(1, 1 + len(ev))))) * 100 + 10 * ef.Q\n",
    "\n",
    "# if params['folds'] in ['B', 'C', ]:\n",
    "for t, v in zip(ef.itertuples(), s):\n",
    "    etables.extend([(t.Index, v)] * (\n",
    "        int(round(t.Length) ** (0.5 if params['folds'] in ['C', 'D'] else 1))\n",
    "                if params['folds'] in ['B', 'C', 'D'] else 1\n",
    "                ))   \n",
    "\n",
    "etable = pd.Series(*list(zip(*etables))[::-1])\n",
    "etable = pd.concat((etable, #*([etable[etable % 100 > 0]] * 3), *([etable[etable % 100  > 10]] * 10), \n",
    "                    pd.Series(0, list(set(metadata.index) - set(etable.index)))))\n",
    "etable = (etable + 1 * etable.index.isin(tdcsfog_metadata.Id)).astype(int)\n",
    "esubject = msubject.reindex(etable.index)\n",
    "etable.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "\n",
    "folds = list(StratifiedGroupKFold(n_splits = N_FOLDS, \n",
    "                                shuffle = True, random_state = SEED\n",
    "        ).split(np.zeros(len(etable)), etable, groups = esubject))\n",
    "train_fold, test_fold = folds[FOLD]\n",
    "train_ids = etable.iloc[train_fold].index\n",
    "test_ids = etable.iloc[test_fold].index\n",
    "assert set(msubject.loc[train_ids]) & set(msubject.loc[test_ids]) == set()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_ids), len(test_ids))\n",
    "\n",
    "train_subjects = msubject.loc[train_ids].unique()\n",
    "test_subjects = msubject.loc[test_ids].unique()\n",
    "assert set(train_subjects) & set(test_subjects) == set()\n",
    "print(len(train_subjects), len(test_subjects)) \n",
    "\n",
    "# ( etable[etable.index.isin(train_ids)].value_counts(), \n",
    "#  etable[etable.index.isin(test_ids)].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = daily_metadata[daily_metadata.Subject.isin(list(train_subjects))]\n",
    "test_df = daily_metadata[~daily_metadata.Subject.isin(list(train_subjects))]\n",
    "\n",
    "train_daily_ids, test_daily_ids = train_df.Id.tolist(), test_df.Id.tolist()\n",
    "train_daily_subjects, test_daily_subjects = train_df.Subject, test_df.Subject\n",
    "assert set(train_daily_ids) & set(test_daily_ids) == set()\n",
    "assert set(train_daily_subjects) & set(test_daily_subjects) == set()\n",
    "print(len(train_daily_ids), len(test_daily_ids))\n",
    "print(len(train_daily_subjects), len(test_daily_subjects))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2 = subjects[subjects.Subject.isin(daily_metadata.Subject) & (subjects.NFOGQ == 0)].Subject.unique()\n",
    "assert len(set(s2) & set(train_daily_subjects)) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fog_files = [f for f in files if 'train/' in f and 'fog' in f]# and not any([z in f for z in common_files])]\n",
    "print(len(fog_files))\n",
    "\n",
    "unlabeled_files = [f for f in files if 'unlabeled/' in f]# and not any([z in f for z in common_files])]\n",
    "print(len(unlabeled_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_dict = dict(zip(sorted(tasks.Task.unique()), np.arange(1, 1 + len(tasks.Task.unique()))))\n",
    "len(task_dict)\n",
    "# task_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and process data\n",
    "def load(f):\n",
    "    # zd = zstd.ZstdDecompressor()\n",
    "    # with open(DATA_PATH + 'cache/' + f + '.zstd', 'rb') as f:\n",
    "    #     return pickle.loads(zd.decompress(f.read()))   \n",
    "    return np.load(DATA_PATH + 'cache/' + f + '.npy')\n",
    "\n",
    "def process(f):\n",
    "    # if exists, return stats\n",
    "    cache_file = DATA_PATH + 'cache/' + f + '.npy'\n",
    "    if os.path.exists(cache_file) and os.path.getsize(cache_file) > 0: \n",
    "        return load(f).shape, os.path.getsize(cache_file)\n",
    "    \n",
    "    # if not, load array,\n",
    "    df = pd.read_csv(getFile(f, ), )\n",
    "\n",
    "    # verify\n",
    "    assert (df.Time == df.index).all()\n",
    "    assert all(df.columns == ['Time',\n",
    "                           'AccV', 'AccML', 'AccAP',\n",
    "                           'StartHesitation', 'Turn', 'Walking',\n",
    "                            'Valid', 'Task'][:len(df.columns)])\n",
    "    assert len(df.columns) in [7, 9]\n",
    "    \n",
    "    v = np.zeros((len(df), 12), dtype = np.float32)\n",
    "    v[:, 6:8] = 1\n",
    "    v[:, :df.shape[1] - 1] = df.iloc[:, 1:]\n",
    "\n",
    "    fid = f.split('/')[-1].split('.')[0]\n",
    "    mult = 100 if 'tdcs' not in f else 128\n",
    "    for e in events[events.Id == fid].itertuples():\n",
    "        v[int(round(e.Init * mult)): int(round(e.Completion * mult)), 8] = 1\n",
    "        v[int(round(e.Init * mult)): int(round(e.Completion * mult)), 9] = e.Kinetic\n",
    "        v[int(round(e.Init * mult)): int(round(e.Completion * mult)), 10] = 1 - e.Kinetic\n",
    "    for e in tasks[tasks.Id == fid].itertuples():\n",
    "        v[int(round(e.Begin * mult)): int(round(e.End * mult)), 11] = task_dict[e.Task]\n",
    "\n",
    "    # store as compresssed;\n",
    "    assert v.dtype == np.float32\n",
    "    # zc = zstd.ZstdCompressor()\n",
    "    # compr = zc.compress(pickle.dumps(v))\n",
    "    os.makedirs(os.path.dirname(cache_file), exist_ok = True)\n",
    "    np.save(cache_file, v)\n",
    "    # with open(cache_file, 'wb') as f:\n",
    "    #     f.write(compr)\n",
    "\n",
    "    return v.shape, os.path.getsize(cache_file)#len(compr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "sz = 0\n",
    "epreds = []#defaultdict(list)\n",
    "paginator = s3.get_paginator('list_objects_v2')\n",
    "for page in paginator.paginate(Bucket = BUCKET, Prefix = PREFIX + 'epreds/'):\n",
    "    for obj in page['Contents']:\n",
    "        epreds.append(obj['Key'].split('/')[-1].split('.')[0])\n",
    "len(epreds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_ereds(x):\n",
    "    # x = np.concatenate([x, np.zeros((x.shape[0], 1))], 1)\n",
    "    savg = pd.Series(x[:, -1]).rolling(200, center = True, min_periods = 1).mean()\n",
    "    x[:, -1:] -= max(0.2, np.quantile(savg, 0.15))\n",
    "    x = x.clip(0, None) #+ 0.01\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "while True:\n",
    "    x = pickle.loads(zstd.decompress(\n",
    "        s3.get_object(Bucket = BUCKET, \n",
    "            Key = PREFIX + 'epreds/' + random.choice(epreds) + '.pkl.zstd')['Body'].read()\n",
    "    ))\n",
    "    x = clip_ereds(x)\n",
    "    srate = 10 * x[:, -1].mean()\n",
    "    \n",
    "    if random.random() > srate: continue;    \n",
    "    print(100 * srate); \n",
    "    plt.plot(x[:5000, ], alpha = 0.5, linewidth = 2);\n",
    "    plt.ylim(0, 1.)\n",
    "    break;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_epreds(ep):\n",
    "    s3 = boto3.client('s3')\n",
    "    return pickle.loads(zstd.decompress(\n",
    "        s3.get_object(Bucket = DATA_BUCKET, \n",
    "            Key = PREFIX + 'epreds/' + random.choice(epreds) + '.pkl.zstd')['Body'].read()\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DAILY_SPLIT = 100000 #if not RELABEL else 50000\n",
    "\n",
    "def processDaily(f, relabel = False, ):\n",
    "    cache_file = DATA_PATH + 'cache/{}_{:05d}'.format(f, 100) + '.npy'\n",
    "    if os.path.exists(cache_file) and os.path.getsize(cache_file) > 0: \n",
    "        return os.path.getsize(cache_file)\n",
    "    \n",
    "    df = pd.read_parquet(getFile(f), columns  = ['AccV', 'AccML', 'AccAP']\n",
    "                                ).astype(np.float32).round(3)\n",
    "    os.remove(DATA_PATH + 'data/' + f + '.zstd')\n",
    "    \n",
    "    assert all(df.columns == ['AccV', 'AccML', 'AccAP',])\n",
    "    assert df[::100].std().mean() > 0.1\n",
    "    v = df.values\n",
    "\n",
    "    maxlen = DAILY_SPLIT\n",
    "    zc = zstd.ZstdCompressor()\n",
    "    os.makedirs(os.path.dirname(cache_file), exist_ok = True)\n",
    "    klast = None; plast = None    \n",
    "    for i in range(0, math.ceil(len(v) / maxlen)):\n",
    "        vsplit = v[i * maxlen:(i + 1) * maxlen]\n",
    "        if relabel:\n",
    "            k = '{}_{:05d}'.format(unlabeled_files[0].split('/')[-1].split('.')[0], i // 10) \n",
    "            if k != klast:\n",
    "                # print(k)\n",
    "                labels = load_epreds(k)\n",
    "                labels = clip_ereds(labels)\n",
    "                labels = cv2.resize(labels, None, fx = 1, fy = 10)\n",
    "                klast = k; plast = labels\n",
    "            vsave = np.concatenate([vsplit, plast[i % 10 * (maxlen)\n",
    "                                                    : (i % 10 + 1) * (maxlen )]\n",
    "                                                [:len(vsplit)]], 1)\n",
    "            # print(vsave.shape, vsave.std(0)[3:])\n",
    "        cache_file = DATA_PATH + 'cache/{}_{:05d}'.format(f, i) + '.npy'\n",
    "        # compr = zc.compress(pickle.dumps(vsave))\n",
    "        np.save(cache_file, vsave.astype(np.float16))\n",
    "\n",
    "        # with open(cache_file, 'wb') as fc:\n",
    "            # fc.write(compr)\n",
    "    return v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDaily(f, i0, i1, verbose = False):\n",
    "    vs = []\n",
    "    # print(f, i0, i1)\n",
    "    if 'unlabeled/' not in f: f = 'unlabeled/' + f\n",
    "    if 'cache/' in f: f = f.replace('cache/', '')  \n",
    "    if '.parquet' not in f: f = f + '.parquet'\n",
    "    # print(f)\n",
    "    cmin, cmax = i0 // DAILY_SPLIT, (i1 - 1) // DAILY_SPLIT\n",
    "    for i in range(cmin, cmax + 1):\n",
    "        cache_file = DATA_PATH + 'cache/{}_{:05d}'.format(f, i) + '.npy'\n",
    "        if verbose: print(cache_file)\n",
    "        if not os.path.exists(cache_file): break;\n",
    "        \n",
    "        with open(cache_file, 'rb') as fc:\n",
    "            vs.append(\n",
    "                # pickle.loads(zd.decompress(fc.read()))\n",
    "                np.load(cache_file)#.astype(np.float32)\n",
    "                )\n",
    "    v = (vs[0] if len(vs) == 1 else np.concatenate(vs)).astype(np.float32)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -r /data/cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit -n 1\n",
    "# loadDaily(unlabeled_files[0], 0, 100000).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit -n 10\n",
    "# loadDaily(unlabeled_files[0], 0, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # processDaily(unlabeled_files[0], RELABEL)\n",
    "# x = loadDaily(unlabeled_files[0], 2100000, 2190000)\n",
    "# plt.plot(x[:, :3], alpha = 0.5, linewidth = 0.3);\n",
    "# plt.plot(x[:, 3:] * 5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # compile dailies\n",
    "# # paginator for s3, list_objects_v2, call\n",
    "# from collections import defaultdict\n",
    "# sz = 0\n",
    "# label_objs = defaultdict(list)\n",
    "# paginator = s3.get_paginator('list_objects_v2')\n",
    "# for page in paginator.paginate(Bucket = BUCKET, Prefix = PREFIX + 'spreds/'):\n",
    "#     for obj in page['Contents']:\n",
    "#         if (obj['Key'].endswith('.pkl.zstd')\n",
    "#             and obj['LastModified'] > #datetime.datetime(2023, 6, 8, 12, 0)):\n",
    "#                 datetime.datetime(2023, 6, 8, 13, 0, tzinfo=datetime.timezone.utc)):\n",
    "#             label_objs[obj['Key'].split('/')[-1].split('.')[0]].append(obj['Key'])\n",
    "#             sz += obj['Size']\n",
    "#         else:\n",
    "#             # delete\n",
    "#             # s3.delete_object(Bucket = BUCKET, Key = obj['Key'])\n",
    "#             print(obj['LastModified'])#x')\n",
    "#     # break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lens = [len(v) for v in label_objs.values()]\n",
    "# len(lens), sum(lens), max(lens), min(lens), np.mean(lens), np.median(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max([len(set([e.split('/')[-2] for e in o]))\n",
    "#         for o in label_objs.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_groups = [(k, v) for k, v in label_objs.items() \n",
    "#                if len(v) >= 72/4\n",
    "#                and k not in epreds\n",
    "#                ]\n",
    "# len(full_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_obj(k):\n",
    "#     s3 = boto3.client('s3')\n",
    "#     return pickle.loads(zstd.decompress(\n",
    "#         s3.get_object(Bucket = BUCKET, Key = k)['Body'].read()\n",
    "#     ))\n",
    "\n",
    "# def compile_preds(k, v):\n",
    "#     s3 = boto3.client('s3')\n",
    "#     r = [load_obj(k) for k in v]\n",
    "#     assert len(r) >= 72/4\n",
    "#     avg = np.stack(r).mean(0).round(3)\n",
    "#     s3.put_object(Bucket = BUCKET, Key = PREFIX + 'epreds/' + k + '.pkl.zstd',\n",
    "#                 Body = zstd.compress(pickle.dumps(avg)))\n",
    "#     return len(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r = Parallel(n_jobs = os.cpu_count() * 3)(\n",
    "#     delayed(compile_preds)(k, v) for k, v in full_groups[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !sudo apt-get install zip\n",
    "# !zip -r backup/backup-inprogress2.zip *.py *.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# process cache\n",
    "r = Parallel(n_jobs = 3)(delayed(processDaily)(f, RELABEL) \n",
    "    for f in unlabeled_files \n",
    "    if any([z in f for z in ( train_daily_ids if RELABEL\n",
    "            else random.sample(train_daily_ids, k = min(10, len(train_daily_ids))) )\n",
    "                             + test_daily_ids])\n",
    "    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the last file (sorted), based on prefix prior to ., for a list\n",
    "uc = 'cache/unlabeled/'\n",
    "f = sorted([f for f in os.listdir(DATA_PATH + uc)\n",
    "                        # if any([z in f for z in daily_ids])\n",
    "                ], )#key = lambda x: x.split('.')[0].split('_')[-1])\n",
    "ucount = {}\n",
    "for e in f: ucount[uc + e.split('.')[0]] = (int(e.split('_')[-1].split('.')[0]) + 1) * DAILY_SPLIT\n",
    "if RELABEL: assert len(ucount) == len(unlabeled_files) \n",
    "sum(ucount.values()) / 1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!du -sh {DATA_PATH}cache/unlabeled*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# process all files\n",
    "r = Parallel(os.cpu_count())(delayed(process)(f) for f in fog_files[:])\n",
    "\n",
    "# display counts;\n",
    "lcount = dict(zip(fog_files, [e[0][0] for e in r]))\n",
    "[sum([v for k, v in lcount.items() if s in k]) / 1e6\n",
    "        for s in ['/defog/', '/tdcsfog/', ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!du -sh {DATA_PATH}cache/train*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATCH = params.get('patch', 12)\n",
    "SEQ = params.get('seq', 256)\n",
    "PATCH * SEQ / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i dataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = WalkDataset(    \n",
    "    {k: v for k, v in lcount.items()\n",
    "                if k.split('/')[-1].split('.')[0] in train_ids\n",
    "                and k.split('/')[-1].split('.')[0] not in drop_ids\n",
    "                }, \n",
    "                metadata, load, loadDaily, -1, id_frac = id_frac,\n",
    "                test = False, **getParams(WalkDataset, params))\n",
    "pure_train_data = train_data\n",
    "\n",
    "test_data = WalkDataset(\n",
    "    {k: v for k, v in lcount.items()\n",
    "                if k.split('/')[-1].split('.')[0] in test_ids},\n",
    "                  metadata, load, loadDaily, -1,\n",
    "                  test = True,# id_frac = test_id_frac,\n",
    "                  **getParams(WalkDataset, params))\n",
    "train_daily_data = WalkDataset(\n",
    "    {k: v for k, v in ucount.items()\n",
    "                if k.split('/')[-1] in train_daily_ids},\n",
    "                dmetadata if RELABEL else metadata, load, loadDaily, DAILY_SPLIT,\n",
    "                test = False,\n",
    "                **getParams(WalkDataset, params))\n",
    "test_daily_data = WalkDataset(\n",
    "    {k: v for k, v in ucount.items()\n",
    "                if k.split('/')[-1] in test_daily_ids},\n",
    "                metadata, load, loadDaily, DAILY_SPLIT,\n",
    "                test = True,\n",
    "                **getParams(WalkDataset, params)) \n",
    "\n",
    "len(train_data), len(test_data), len(train_daily_data), len(test_daily_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComboDataset(Dataset):\n",
    "    ''' combines two datasets, all of the first one \n",
    "        plus a fraction of the second one, specified as pct of size of first one\n",
    "\n",
    "        always use random idxs for the second one, so that it's not biased\n",
    "\n",
    "    '''\n",
    "    def __init__(self, d1, d2, d2_frac = 0.5):\n",
    "        self.d1 = d1\n",
    "        self.d2 = d2\n",
    "        self.d2_frac = d2_frac\n",
    "        self.d1_len = len(d1)\n",
    "        self.d2_len = int(self.d1_len * d2_frac)\n",
    "        self.len = self.d1_len + self.d2_len\n",
    "        self.d1_len, self.d2_len, self.len\n",
    "        self.d2_idxs = np.random.choice(len(d2), self.d2_len, replace = False)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if idx < self.d1_len:\n",
    "            return self.d1[idx]\n",
    "        else:\n",
    "            return self.d2[self.d2_idxs[idx - self.d1_len]]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineMixDataset(Dataset):\n",
    "    def __init__(self, datasetA, datasetB, n = None):\n",
    "        self.datasetA = datasetA\n",
    "        self.datasetB = datasetB\n",
    "        self.n = n or max(len(self.datasetA), len(self.datasetB))        \n",
    "        # self.current_idx = 0\n",
    "        self.aidxs = np.arange(len(self.datasetA))\n",
    "        self.bidxs = np.arange(len(self.datasetB))\n",
    "        np.random.shuffle(self.aidxs)\n",
    "        np.random.shuffle(self.bidxs)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n\n",
    "    \n",
    "    def __getitem__(self, idx):        \n",
    "        # self.current_idx += 1\n",
    "        if random.random() < 0.5 * (1 + np.cos(min(1, idx / self.n) * np.pi)):\n",
    "            return self.datasetA[self.aidxs[idx % len(self.datasetA)]]\n",
    "        else:\n",
    "            return self.datasetB[self.bidxs[idx % len(self.datasetB)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RELABEL:\n",
    "    train_data = CosineMixDataset(train_daily_data, pure_train_data,\n",
    "                                    params['batch_size'] * params['steps']) \n",
    "\n",
    "else:\n",
    "    train_data = ComboDataset(train_data, train_daily_data, 0.2)\n",
    "\n",
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, s, frac = train_data[random.choice(range(len(train_data)))][:4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WalkModule(params, **getParams(WalkModule, params)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(params)\n",
    "model.train()\n",
    "from torch.utils.data._utils.collate import default_collate\n",
    "x, y, s, frac, m, f, i, flen = default_collate(\n",
    "    [train_data[random.choice(range(len(train_data)))] for i in range(32)])\n",
    "\n",
    "yp = model(x.to(device),\n",
    "            m.to(device),\n",
    "            frac.to(device),\n",
    "            flen.to(device),\n",
    "            # adjust = False\n",
    "            )[0]\n",
    "loss = yp.mean()\n",
    "loss.backward()\n",
    "print(yp.mean().item(), yp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{k: v for k,v in params.items() if ('wt' in k) and '_' in k}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{k: v for k,v in params.items() if ('se' in k or 'frac' in k or 'adj' in k or 'm_' in k) and '_' in k}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size = params['batch_size'], \n",
    "                            shuffle = True if not RELABEL else False,\n",
    "                            drop_last = True,\n",
    "                              num_workers = os.cpu_count())\n",
    "  \n",
    "test_loader = DataLoader(test_data, batch_size = 32,\n",
    "                          num_workers = os.cpu_count())\n",
    "test_daily_loader = DataLoader(test_daily_data, batch_size = 32,\n",
    "                          num_workers = os.cpu_count())\n",
    "\n",
    "len(train_loader), len(test_loader), len(test_daily_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RELABEL:\n",
    "    model.params.steps = len(train_loader)\n",
    "else:\n",
    "    model.params.steps = int(params['step_mult'] * (len(train_data) * len(train_loader)) ** 0.5)\n",
    "\n",
    "model.params.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and show progress bar every 10 steps\n",
    "m = secrets.token_hex(3); m\n",
    "trainer = pl.Trainer(accelerator = 'auto', logger = False, \n",
    "                     precision = 16,\n",
    "                     gradient_clip_val = 1,\n",
    "                        enable_checkpointing = False,\n",
    "                        val_check_interval = model.params.steps,\n",
    "                        check_val_every_n_epoch = None,\n",
    "                        max_steps = model.params.steps,\n",
    "                        callbacks = [pl.callbacks.TQDMProgressBar(refresh_rate = 5)],\n",
    "                        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model, train_loader, test_loader) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TARGETS = 4\n",
    "STORE_SUBSAMPLE = 10\n",
    "STORE_SPLIT = 100000\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def stack_store(yps, fs, idxs):\n",
    "    _fs = np.concatenate(fs)\n",
    "    _idxs = torch.cat(idxs).cpu().numpy()    \n",
    "    _yps = torch.cat(yps).cpu().numpy()\n",
    "    _yps[..., 3] = _yps[..., 3:5].mean(-1)\n",
    "    _yps = _yps[..., :N_TARGETS]        \n",
    "\n",
    "    assert all([len(e) == len(_yps) for e in [_fs, _idxs, _yps]])\n",
    "    return _yps, _fs, _idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_store(_yps, _fs, _idxs, pred_dict, ct_dict, age_dict):        \n",
    "    for i in range(len(_yps)):\n",
    "        ridxs = np.arange(_idxs[i], _idxs[i] + len(_yps[i]))\n",
    "        for ig in np.unique(ridxs // STORE_SPLIT):\n",
    "            k = (_fs[i], ig )\n",
    "            _ = ridxs // STORE_SPLIT == ig\n",
    "            aidx = ridxs[_] % STORE_SPLIT\n",
    "            # print(k, _yps[i].shape, aidx.shape, )\n",
    "            pred_dict.setdefault(k, np.zeros((STORE_SPLIT, N_TARGETS), dtype = np.float32))\n",
    "            ct_dict.setdefault(k, np.zeros((STORE_SPLIT, N_TARGETS), dtype = np.float32))\n",
    "            pred_dict[k][aidx] += _yps[i][_] \n",
    "            ct_dict[k][aidx] += np.ones_like(_yps[i][_])\n",
    "            age_dict[k] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "\n",
    "def s3_push(b, k, body):\n",
    "    s3.put_object(Bucket = b, Key = k, Body = body)\n",
    "    \n",
    "def flush_store(pred_dict, ct_dict, age_dict):\n",
    "    for k in list(ct_dict.keys()):\n",
    "        if ( ( (ct_dict[k] >= 1).mean() >= 1. )\n",
    "             or (ct_dict[k] >= 1).mean() >= 0.1 \n",
    "                    and age_dict[k] > 10):\n",
    "            x = pred_dict[k] / (ct_dict[k] + 1e-5)\n",
    "            compr = zc.compress(pickle.dumps(x.astype(np.float32).round(3)))\n",
    "            key = (PREFIX + 'spreds{}/'.format(2 if RELABEL else '') +  '{}/'.format(m)\n",
    "                 + '{}_{:05d}'.format(k[0].split('/')[-1], k[1],) + '.pkl' + '.zstd')\n",
    "            Thread(target = s3_push, args = (BUCKET, key, compr)).start()\n",
    "\n",
    "            # s3.put_object(Bucket = BUCKET, Key = key, Body = compr)\n",
    "            print(key, len(compr), ct_dict[k].mean(), age_dict[k], x.std())\n",
    "            pred_dict.pop(k, None), ct_dict.pop(k, None), age_dict.pop(k, None)\n",
    "        \n",
    "        elif age_dict[k] > 20:\n",
    "            age_dict[k] = 1 + age_dict.get(k, 0)\n",
    "            if age_dict[k] > 10:\n",
    "                pred_dict.pop(k, None), ct_dict.pop(k, None), age_dict.pop(k, None)\n",
    "                print('del', k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = random.choice(list(model.pred_dict))\n",
    "plt.plot(model.pred_dict[k] / model.ct_dict[k])\n",
    "plt.plot(model.target_dict[k])\n",
    "plt.ylim(0, 1.03);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: s3.put_object(Bucket = BUCKET, \n",
    "              Key = PREFIX + 'preds/' + m + '.pkl' + '.zstd', \n",
    "              Body = zc.compress(pickle.dumps((model.pred_dict, model.target_dict, model.ct_dict))))\n",
    "except Exception as e: print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: s3.put_object(Bucket = BUCKET, \n",
    "              Key = PREFIX + 'models/' + m + '.pt', \n",
    "              Body = pickle.dumps(model.model.state_dict()))\n",
    "except Exception as e: print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {} \n",
    "r['params'] = params\n",
    "r['results'] = {k: v.item() if torch.is_tensor(v) else v \n",
    "                for k, v in trainer.callback_metrics.items()}\n",
    "k = PREFIX + 'results/{}.json'.format(m)\n",
    "s3.put_object(Bucket = BUCKET, \n",
    "              Key = k,\n",
    "              Body = json.dumps(r), \n",
    "              )\n",
    "k, r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_dict, ct_dict, age_dict = {}, {}, defaultdict(int)\n",
    "# # model = net\n",
    "# ct = 0\n",
    "# model.to(device)\n",
    "# model.eval();\n",
    "\n",
    "# xs, yps, fs, idxs = [], [], [], []\n",
    "# iprior = None; fprior = None\n",
    "# for batch in test_daily_loader:\n",
    "#     x, y, s, frac, m_, f, i, flen = batch\n",
    "#     with torch.no_grad():\n",
    "#         yp, ypae, ypm = model(*[e.to(device)\n",
    "#                                  for e in [x, m_, frac, flen]], adjust = False)\n",
    "    \n",
    "#     yps.append(yp[:, ::STORE_SUBSAMPLE].cpu())\n",
    "#     # xs.append(x.cpu()); \n",
    "#     fs.append(f)\n",
    "#     idxs.append(i // STORE_SUBSAMPLE)\n",
    "#     if len(yps) >= 100: break;\n",
    "\n",
    "#     if ((iprior is not None and not ( iprior == i // (STORE_SPLIT * STORE_SUBSAMPLE)).all())\n",
    "#         or (fprior is not None and not all([fprior == e for e in f]))):\n",
    "    \n",
    "#         _yps, _fs, _idxs = stack_store(yps, fs, idxs)\n",
    "#         process_store(_yps, _fs, _idxs, pred_dict, ct_dict, age_dict)\n",
    "#         flush_store(pred_dict, ct_dict, age_dict)\n",
    "#         yps, fs, idxs = [], [], []\n",
    "#         ct += 1\n",
    "#         # break;\n",
    "#         # if fprior is not None and not all([fprior == e for e in f]):\n",
    "#         #     break;\n",
    "    \n",
    "#     iprior = i[-1].item() // (STORE_SPLIT * STORE_SUBSAMPLE)\n",
    "#     fprior = f[-1] \n",
    "\n",
    "# if len(yps) > 0:\n",
    "#     _yps, _fs, _idxs = stack_store(yps, fs, idxs)\n",
    "#     yps, fs, idxs = [], [], []\n",
    "#     process_store(_yps, _fs, _idxs, pred_dict, ct_dict, age_dict)\n",
    "    \n",
    "# for i in range(100): flush_store(pred_dict, ct_dict, age_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: sqs.delete_message(QueueUrl = queue_url, ReceiptHandle = rh)\n",
    "except Exception as e: print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo shutdown now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime, json\n",
    "paginator = s3.get_paginator('list_objects_v2')\n",
    "page_iterator = paginator.paginate(Bucket = BUCKET, Prefix = PREFIX + 'results/')\n",
    "def flatten(l): return [item for sublist in l for item in sublist]\n",
    "keys = flatten([[k['Key'] for k in page['Contents']\n",
    "                    if k['LastModified'].replace(tzinfo = None) \n",
    "                            > datetime.datetime.now() - datetime.timedelta(hours = 4.3  )\n",
    "                    # and k['LastModified'].replace(tzinfo = None) \n",
    "                #             < datetime.datetime.now() - datetime.timedelta(hours = 0.2)\n",
    "                    # if k['LastModified'].replace(tzinfo = None) \n",
    "                    #         >= datetime.datetime(2023, 6, 6, 5, 12, 18,) # pseudos were here;\n",
    "                    # if k['LastModified'].replace(tzinfo = None) \n",
    "                    #         <= datetime.datetime(2023, 6, 6, 10, 12, 18,)\n",
    "                 ] for page in page_iterator])\n",
    "def loadObj(k):\n",
    "    s3 = boto3.client('s3')\n",
    "    o = s3.get_object(Bucket = BUCKET, Key = k)\n",
    "    # print(o)\n",
    "    r = json.loads(o['Body'].read())\n",
    "    r['m'] = k.split('/')[-1].split('.')[0]\n",
    "    r['t'] = o['LastModified']\n",
    "    return r\n",
    "results = Parallel(os.cpu_count() * 3)(delayed(loadObj)(k) for k in keys)\n",
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "presults = defaultdict(list)\n",
    "ms = {}; pms = defaultdict(list)\n",
    "# f = random.choice(results)['params']['frac_pwr_mult']\n",
    "for r in results:\n",
    "    p = r['params']\n",
    "    \n",
    "    if 'seg' in p and p['seed'] < 400: continue;\n",
    "    # if p['seed'] //100 in [4, 5]: continue;\n",
    "    # if p['seq'] < 224: continue;\n",
    "    # if p['dims'] < 384: continue;\n",
    "    # if p.get('xformer_layers', 2) > 3: continue;\n",
    "    # if p['frac_pwr_mult'] != f: continue\n",
    "    # if p['se_dims'] > 0 and p.get('se_act', '') == 'PReLU': continue;\n",
    "    r_ = {k: v for k, v in r['params'].items() \n",
    "              if k not in ['fold', 'seed', 'n_folds']}\n",
    "    presults[json.dumps(r_)].append(r['results']['val_ap'])\n",
    "    pms[json.dumps(r_)].append(r['m'])\n",
    "    ms[r['m']] = r['params']\n",
    "\n",
    "len(ms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict to df, each element is row\n",
    "df = pd.DataFrame({k: (np.mean(v), np.min(v), np.max(v), len(v))\n",
    "      for k, v in presults.items()}, index = ['mean', 'min', 'max', 'ct']).T.sort_values('mean')[::-1]\n",
    "# df = df.loc[[e for e in df.index \n",
    "#              if not any( [z in e for z in [ 'steps', 'h0', 'patch', 'mult', 'seq', 'layers' ]] ) ]]\n",
    "df = df[df.ct >= 4 ]#params['n_folds'] == 0]\n",
    "# df -= df.loc['{}']#max()\n",
    "df.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(l): return [item for sublist in l for item in sublist]\n",
    "select_ms = flatten([pms[e] for e in df.head(10).index\n",
    "                        # if 'frac_adj\": false' in e\n",
    "                     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "', '.join(select_ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame([json.loads(k) for k in presults])\n",
    "df2 = pd.DataFrame([(np.mean(v), np.min(v), np.max(v), len(v))\n",
    "      for k, v in presults.items()], columns = ['mean', 'min', 'max', 'ct'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 200)\n",
    "pd.concat((df1, df2),axis = 1).sort_values('mean')[::-1][df2.ct >= 3 #df2.ct.max()\n",
    "                                                         ].round(6).iloc[:,50:\n",
    "                                                      # ][[c for c in df1.columns if '_' not  in c]\n",
    "                                                          ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy add two arrays, expanding the smaller one on axis 0\n",
    "def eadd(a, b):\n",
    "    if isinstance(a, int): return b.copy();\n",
    "    if a.shape[0] < b.shape[0]:\n",
    "        a = np.pad(a, ((0, b.shape[0] - a.shape[0]), (0, 0)), mode = 'constant')\n",
    "    elif a.shape[0] > b.shape[0]:\n",
    "        b = np.pad(b, ((0, a.shape[0] - b.shape[0]), (0, 0)), mode = 'constant')\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "zd = zstd.ZstdDecompressor()\n",
    "def loadObj(k):\n",
    "    file_path = DATA_PATH + 'obj_cache/' + k\n",
    "    if not os.path.exists(file_path):\n",
    "        s3 = boto3.client('s3')\n",
    "        o = s3.get_object(Bucket = BUCKET, Key = k)\n",
    "        os.makedirs(os.path.dirname(file_path), exist_ok = True)\n",
    "        with open(file_path, 'wb') as f:\n",
    "            f.write(o['Body'].read())\n",
    "    r = pickle.loads(zd.decompress(open(file_path, 'rb').read()))\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMMON = ['2d57c2', 'e86b6e'][:2]\n",
    "common_files = (tdcsfog_metadata.Id[tdcsfog_metadata.Subject.isin(COMMON)].tolist()\n",
    "                     + defog_metadata.Id[defog_metadata.Subject.isin(COMMON)].tolist())\n",
    "len(common_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit(x): return 1 / (1 + np.exp(-x))\n",
    "def rlogit(x): return -np.log(1/(x * 0.9999 + 1e-4/2) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_totals = {}\n",
    "target_totals = {}\n",
    "ct_totals = {}\n",
    "t_ct_totals = {}\n",
    "scales = []\n",
    "for m, p in ms.items():\n",
    "    try:\n",
    "        if m not in select_ms: continue\n",
    "        r = loadObj(PREFIX + 'preds/' + m + '.pkl.zstd')\n",
    "        mult = 1#/8 if p.get('seg') else 1; #print(mult)\n",
    "\n",
    "        pred, target, ct = r\n",
    "        # print(len(pred))\n",
    "        pred, target, ct = [{k: v for k, v in p.items()\n",
    "                              if not any([c in k for c in  common_files])}\n",
    "                                    for p in [pred, target, ct]]\n",
    "        # print(len(pred))\n",
    "        spred = {k: pred[k] / (ct[k] + 1e-5) for k in pred.keys() }\n",
    "        pred_total = np.stack([e.sum(0) for e in spred.values()]).sum(0)\n",
    "        total = np.stack([e.sum(0) for e in target.values()]).sum(0)        \n",
    "        scale = total / pred_total * mult\n",
    "        scales.append(scale)\n",
    "        print(scale.round(2))\n",
    "\n",
    "        # print(m, pred_total, total, scale.round(2)); #break; \n",
    "        for k, v in spred.items():\n",
    "            pred_totals[k] = eadd( pred_totals.get(k, 0), v * scale ) #\n",
    "                                        # * logit(rlogit(v) + np.log(scale)) )\n",
    "        for k, v in target.items():\n",
    "            target_totals[k] = eadd(target_totals.get(k, 0), v)\n",
    "            t_ct_totals[k] = eadd(t_ct_totals.get(k, 0), 1 * (v > -np.inf))\n",
    "        for k, v in ct.items():\n",
    "            ct_totals[k] = eadd(ct_totals.get(k, 0), (v > 0) * mult)\n",
    "    except Exception as e:\n",
    "        # raise e\n",
    "        print('error', m, e)\n",
    "    # break;        \n",
    "assert ( set(pred_totals.keys()) == set(target_totals.keys()) \n",
    "                == set(ct_totals.keys()) == set(t_ct_totals.keys()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.stack(scales))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = random.choice(list(hcommon_files))#.keys()))\n",
    "k = random.choice(list(pred_totals.keys()))\n",
    "plt.plot(pred_totals[k] / ct_totals[k])\n",
    "plt.plot(target_totals[k] / t_ct_totals[k])\n",
    "plt.ylim(0, 1.05);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HCOMMON = ['2d57c2', 'e86b6e']\n",
    "# hcommon_files = (tdcsfog_metadata.Id[tdcsfog_metadata.Subject.isin(HCOMMON)].tolist()\n",
    "#                      + defog_metadata.Id[defog_metadata.Subject.isin(HCOMMON)].tolist())\n",
    "# hcommon_files = [f for f in fog_files if any([c in f for c in hcommon_files])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = [f for f in ct_totals.keys() if not any([z in f for z in common_files])]\n",
    "print('{} of {}'.format(len(keys), len(ct_totals)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "ct_dict, pred_dict, target_dict = ct_totals, pred_totals, target_totals\n",
    "final_ys, final_yps = [], []\n",
    "for k in keys:\n",
    "    minlen = min([e.shape[0] for e in [ct_dict[k], pred_dict[k], target_dict[k]]])\n",
    "    # print(minlen)\n",
    "    # minlen = (ct_totals[k] > 0).sum()\n",
    "    # print(minlen)\n",
    "    # break;\n",
    "    f = ct_dict[k][:minlen][:, 0] > 0\n",
    "    final_ys.append(1 * (target_dict[k][:minlen][f]/ t_ct_totals[k][:minlen][f] > 0.5))\n",
    "    final_yps.append(pred_dict[k][:minlen][f] / ct_dict[k][:minlen][f])\n",
    "    assert (ct_dict[k].std(1) < 1e-5).all()\n",
    "    assert ct_dict[k][:minlen][f].min() >= 1\n",
    "\n",
    "final_ys, final_yps = np.concatenate(final_ys), np.concatenate(final_yps)\n",
    "\n",
    "aps = []; N = 1\n",
    "labels = 'htw'\n",
    "for i in range(3):\n",
    "    aps.append(average_precision_score(final_ys[::N, i], final_yps[::N,  i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aps, np.mean(aps), len(ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf code/models code/params\n",
    "!mkdir -p code/models code/params\n",
    "def downloadModel(m, p):\n",
    "    # m = r['m'] \n",
    "    s3 = boto3.client('s3')\n",
    "    s3.download_file(Bucket = BUCKET, Key = PREFIX + 'models/' + m + '.pt', \n",
    "                     Filename = 'code/' + 'models/' + m + '.pt')\n",
    "    json.dump(p, open('code/' + 'params/' + m + '.json', 'w'))\n",
    "    \n",
    "r = Parallel(os.cpu_count() * 3)(delayed(downloadModel)(m, p) for m, p in ms.items());\n",
    "# total size of folds\n",
    "!du -sh code/models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls -lh code/models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir -p day2main\n",
    "# !cp *py* day2main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kaggle datasets download stochoshi/walkdata -p code1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls code1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -r code1/whl code\n",
    "!cp -r code1/smp code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# kaggle create dataset in code/\n",
    "# move .py files there\n",
    "# submit as dataset\n",
    "\n",
    "!mkdir -p code\n",
    "json.dump({'title': 'Walk Dataset', \n",
    "           'id': 'stochoshi/walkdata4',\n",
    "           'licenses': [{'name': 'CC0-1.0'}]}, \n",
    "          open('code/dataset-metadata.json', 'w'), indent = 4)\n",
    "# !kaggle datasets init -p code\n",
    "!cp *.py code\n",
    "# !kaggle datasets create -p code  --dir-mode tar\n",
    "!kaggle datasets version -p code -m \"Walk Dataset\" --dir-mode tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !sudo apt-get install zip\n",
    "# !zip -r backup/Final.zip *.py *.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
