{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6376a899",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-06-06T12:45:39.575492Z",
     "iopub.status.busy": "2023-06-06T12:45:39.574573Z",
     "iopub.status.idle": "2023-06-06T12:45:42.685823Z",
     "shell.execute_reply": "2023-06-06T12:45:42.684686Z"
    },
    "papermill": {
     "duration": 3.124013,
     "end_time": "2023-06-06T12:45:42.688791",
     "exception": false,
     "start_time": "2023-06-06T12:45:39.564778",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================\n",
    "# library\n",
    "# ============================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import LayerNorm,TransformerEncoder\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import glob\n",
    "from sklearn.preprocessing import StandardScaler,RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35e922b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-06T12:45:42.706900Z",
     "iopub.status.busy": "2023-06-06T12:45:42.705088Z",
     "iopub.status.idle": "2023-06-06T12:45:42.711647Z",
     "shell.execute_reply": "2023-06-06T12:45:42.710711Z"
    },
    "papermill": {
     "duration": 0.017423,
     "end_time": "2023-06-06T12:45:42.713872",
     "exception": false,
     "start_time": "2023-06-06T12:45:42.696449",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================\n",
    "# constant\n",
    "# ============================\n",
    "SUB_PATH = \"/kaggle/input/tlvmc-parkinsons-freezing-gait-prediction/sample_submission.csv\"\n",
    "DEFOG_DATA_PATH = \"/kaggle/input/tlvmc-parkinsons-freezing-gait-prediction/test/defog/*.csv\"\n",
    "TDCSFOG_DATA_PATH = \"/kaggle/input/tlvmc-parkinsons-freezing-gait-prediction/test/tdcsfog/*.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ce2993d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-06T12:45:42.730066Z",
     "iopub.status.busy": "2023-06-06T12:45:42.729210Z",
     "iopub.status.idle": "2023-06-06T12:45:42.792235Z",
     "shell.execute_reply": "2023-06-06T12:45:42.791222Z"
    },
    "papermill": {
     "duration": 0.073746,
     "end_time": "2023-06-06T12:45:42.794840",
     "exception": false,
     "start_time": "2023-06-06T12:45:42.721094",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================\n",
    "# settings\n",
    "# ============================\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "bs = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59e0608e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-06T12:45:42.811257Z",
     "iopub.status.busy": "2023-06-06T12:45:42.810628Z",
     "iopub.status.idle": "2023-06-06T12:45:42.821305Z",
     "shell.execute_reply": "2023-06-06T12:45:42.820371Z"
    },
    "papermill": {
     "duration": 0.021095,
     "end_time": "2023-06-06T12:45:42.823405",
     "exception": false,
     "start_time": "2023-06-06T12:45:42.802310",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================\n",
    "# settings\n",
    "# ============================\n",
    "tdcsfog_path1 = [f\"/kaggle/input/fog-ex143/ex143_{i}.pth\" for i in range(5)] # len 3000 cv TdcsfogRnnModel cv\n",
    "tdcsfog_path3_1 = [f\"/kaggle/input/fog-ex145/ex145_{i}.pth\" for i in range(5)] # len 3000 TdcsfogRnnModel  \n",
    "tdcsfog_path3_2 = [f\"/kaggle/input/fog-ex146/ex146_{i}.pth\" for i in range(5)] # len 3000 TdcsfogRnnModel \n",
    "tdcsfog_path3_3 = [f\"/kaggle/input/fog-ex147/ex147_{i}.pth\" for i in range(5)] # len 3000 TdcsfogRnnModel cv\n",
    "tdcsfog_path4_1 = [f\"/kaggle/input/fog-ex182/ex182_{i}.pth\" for i in range(5)] # len 3000 TdcsfogRnnModel  \n",
    "tdcsfog_path4_2 = [f\"/kaggle/input/fog-ex183/ex183_{i}.pth\" for i in range(5)] # len 3000 TdcsfogRnnModel \n",
    "tdcsfog_path4_3 = [f\"/kaggle/input/fog-ex184/ex184_{i}.pth\" for i in range(5)] # len 3000 TdcsfogRnnModel cv\n",
    "\n",
    "defog_path2 = [f\"/kaggle/input/fog-ex153/ex153_{i}.pth\" for i in range(5)] # len 30000 defog1 \n",
    "defog_path4 = [f\"/kaggle/input/fog-ex179/ex179_{i}.pth\" for i in range(5)] # len 30000 defog1\n",
    "defog_path5 = [f\"/kaggle/input/fog-ex185/ex185_{i}.pth\" for i in range(5)] # len 30000 defog1\n",
    "defog_path6 = [f\"/kaggle/input/fog-ex204/ex204_{i}.pth\" for i in range(5)] # len 30000 defog2\n",
    "defog_path7 = [f\"/kaggle/input/pd-exp238/fold{i}_best.pth\" for i in [0, 1, 2, 3, 4]]  # len 30000 Defog3Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0dbd610c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-06T12:45:42.840106Z",
     "iopub.status.busy": "2023-06-06T12:45:42.839278Z",
     "iopub.status.idle": "2023-06-06T12:45:42.851285Z",
     "shell.execute_reply": "2023-06-06T12:45:42.850235Z"
    },
    "papermill": {
     "duration": 0.022908,
     "end_time": "2023-06-06T12:45:42.853620",
     "exception": false,
     "start_time": "2023-06-06T12:45:42.830712",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Functions\n",
    "# ============================\n",
    "\n",
    "def preprocess(numerical_array, \n",
    "               mask_array,\n",
    "               ):\n",
    "    \n",
    "    attention_mask = mask_array == 0\n",
    "\n",
    "    return {\n",
    "        'input_data_numerical_array': numerical_array,\n",
    "        'input_data_mask_array': mask_array,\n",
    "        'attention_mask': attention_mask,\n",
    "    }\n",
    "\n",
    "class FogDataset(Dataset):\n",
    "    def __init__(self, numerical_array, \n",
    "                 mask_array,\n",
    "                 train = True, y = None):\n",
    "        self.numerical_array = numerical_array\n",
    "        self.mask_array = mask_array\n",
    "        self.train = train\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.numerical_array)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        data = preprocess(\n",
    "            self.numerical_array[item],\n",
    "            self.mask_array[item],\n",
    "            \n",
    "        )\n",
    "\n",
    "        # Return the processed data where the lists are converted to `torch.tensor`s\n",
    "        if self.train : \n",
    "            return {\n",
    "              'input_data_numerical_array': torch.tensor(data['input_data_numerical_array'],dtype=torch.float32),\n",
    "              'input_data_mask_array':torch.tensor(data['input_data_mask_array'], dtype=torch.long),  \n",
    "              'attention_mask': torch.tensor(data[\"attention_mask\"], dtype=torch.bool),\n",
    "              \"y\":torch.tensor(self.y[item], dtype=torch.float32)\n",
    "               }\n",
    "        else:\n",
    "            return {\n",
    "             'input_data_numerical_array': torch.tensor(data['input_data_numerical_array'],dtype=torch.float32),\n",
    "              'input_data_mask_array':torch.tensor(data['input_data_mask_array'], dtype=torch.long),  \n",
    "              'attention_mask': torch.tensor(data[\"attention_mask\"], dtype=torch.bool),\n",
    "               }\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4a42614",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-06T12:45:42.869693Z",
     "iopub.status.busy": "2023-06-06T12:45:42.869394Z",
     "iopub.status.idle": "2023-06-06T12:45:42.892410Z",
     "shell.execute_reply": "2023-06-06T12:45:42.891437Z"
    },
    "papermill": {
     "duration": 0.033659,
     "end_time": "2023-06-06T12:45:42.894660",
     "exception": false,
     "start_time": "2023-06-06T12:45:42.861001",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ================================\n",
    "# tdcsfog\n",
    "# ================================\n",
    "class TdcsfogRnnModel(nn.Module):\n",
    "    def __init__(\n",
    "        self, dropout=0.2,\n",
    "        input_numerical_size=12,\n",
    "        numeraical_linear_size = 64,\n",
    "        model_size = 128,\n",
    "        linear_out = 128,\n",
    "        out_size=3):\n",
    "        super(TdcsfogRnnModel, self).__init__()\n",
    "        self.numerical_linear  = nn.Sequential(\n",
    "                nn.Linear(input_numerical_size, numeraical_linear_size),\n",
    "                nn.LayerNorm(numeraical_linear_size)\n",
    "            )\n",
    "        \n",
    "        self.rnn = nn.GRU(numeraical_linear_size, model_size,\n",
    "                            num_layers = 2, \n",
    "                            batch_first=True,\n",
    "                            bidirectional=True)\n",
    "        self.linear_out  = nn.Sequential(\n",
    "                nn.Linear(model_size*2, \n",
    "                          linear_out),\n",
    "                nn.LayerNorm(linear_out),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(linear_out, \n",
    "                          out_size))\n",
    "        self._reinitialize()\n",
    "        \n",
    "    def _reinitialize(self):\n",
    "        \"\"\"\n",
    "        Tensorflow/Keras-like initialization\n",
    "        \"\"\"\n",
    "        for name, p in self.named_parameters():\n",
    "            if 'rnn' in name:\n",
    "                if 'weight_ih' in name:\n",
    "                    nn.init.xavier_uniform_(p.data)\n",
    "                elif 'weight_hh' in name:\n",
    "                    nn.init.orthogonal_(p.data)\n",
    "                elif 'bias_ih' in name:\n",
    "                    p.data.fill_(0)\n",
    "                    # Set forget-gate bias to 1\n",
    "                    n = p.size(0)\n",
    "                    p.data[(n // 4):(n // 2)].fill_(1)\n",
    "                elif 'bias_hh' in name:\n",
    "                    p.data.fill_(0)\n",
    "    \n",
    "    def forward(self, numerical_array,\n",
    "                mask_array,\n",
    "                attention_mask):\n",
    "        \n",
    "        numerical_embedding = self.numerical_linear(numerical_array)\n",
    "        output,_ = self.rnn(numerical_embedding)\n",
    "        output = self.linear_out(output)\n",
    "        return output\n",
    "    \n",
    "    \n",
    "class TdcsfogRnnModel2(nn.Module):\n",
    "    def __init__(\n",
    "        self, dropout=0.2,\n",
    "        input_numerical_size=12,\n",
    "        numeraical_linear_size = 64,\n",
    "        model_size = 128,\n",
    "        linear_out = 128,\n",
    "        out_size=3):\n",
    "        super(TdcsfogRnnModel2, self).__init__()\n",
    "        self.numerical_linear  = nn.Sequential(\n",
    "                nn.Linear(input_numerical_size, numeraical_linear_size),\n",
    "                nn.LayerNorm(numeraical_linear_size)\n",
    "            )\n",
    "        \n",
    "        self.rnn = nn.GRU(numeraical_linear_size, model_size,\n",
    "                            num_layers = 2, \n",
    "                            batch_first=True,\n",
    "                            bidirectional=True)\n",
    "        self.linear_out  = nn.Sequential(\n",
    "                nn.Linear(model_size*2, \n",
    "                          linear_out),\n",
    "                nn.LayerNorm(linear_out),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout))\n",
    "        self.out1 = nn.Linear(linear_out, \n",
    "                          out_size)\n",
    "        self.out2 = nn.Linear(linear_out, \n",
    "                          out_size)\n",
    "\n",
    "        \n",
    "        self._reinitialize()\n",
    "        \n",
    "    def _reinitialize(self):\n",
    "        \"\"\"\n",
    "        Tensorflow/Keras-like initialization\n",
    "        \"\"\"\n",
    "        for name, p in self.named_parameters():\n",
    "            if 'rnn' in name:\n",
    "                if 'weight_ih' in name:\n",
    "                    nn.init.xavier_uniform_(p.data)\n",
    "                elif 'weight_hh' in name:\n",
    "                    nn.init.orthogonal_(p.data)\n",
    "                elif 'bias_ih' in name:\n",
    "                    p.data.fill_(0)\n",
    "                    # Set forget-gate bias to 1\n",
    "                    n = p.size(0)\n",
    "                    p.data[(n // 4):(n // 2)].fill_(1)\n",
    "                elif 'bias_hh' in name:\n",
    "                    p.data.fill_(0)\n",
    "    \n",
    "    def forward(self, numerical_array,\n",
    "                mask_array,\n",
    "                attention_mask):\n",
    "        \n",
    "        numerical_embedding = self.numerical_linear(numerical_array)\n",
    "        output,_ = self.rnn(numerical_embedding)\n",
    "        output = self.linear_out(output)\n",
    "        output1 = self.out1(output)\n",
    "        output2 = self.out2(output)\n",
    "        return output1,output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a845a1c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-06T12:45:42.910847Z",
     "iopub.status.busy": "2023-06-06T12:45:42.910567Z",
     "iopub.status.idle": "2023-06-06T12:45:42.940545Z",
     "shell.execute_reply": "2023-06-06T12:45:42.939460Z"
    },
    "papermill": {
     "duration": 0.040854,
     "end_time": "2023-06-06T12:45:42.942806",
     "exception": false,
     "start_time": "2023-06-06T12:45:42.901952",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DefogRnnModel(nn.Module):\n",
    "    def __init__(\n",
    "        self, dropout=0.2,\n",
    "        input_numerical_size=9,\n",
    "        numeraical_linear_size = 64,\n",
    "        model_size = 128,\n",
    "        linear_out = 128,\n",
    "        out_size=3):\n",
    "        super(DefogRnnModel, self).__init__()\n",
    "        self.numerical_linear  = nn.Sequential(\n",
    "                nn.Linear(input_numerical_size, numeraical_linear_size),\n",
    "                nn.LayerNorm(numeraical_linear_size)\n",
    "            )\n",
    "        \n",
    "        self.rnn = nn.GRU(numeraical_linear_size, model_size,\n",
    "                            num_layers = 2, \n",
    "                            batch_first=True,\n",
    "                            bidirectional=True)\n",
    "        self.linear_out  = nn.Sequential(\n",
    "                nn.Linear(model_size*2, \n",
    "                          linear_out),\n",
    "                nn.LayerNorm(linear_out),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(linear_out, \n",
    "                          out_size))\n",
    "        self._reinitialize()\n",
    "        \n",
    "    def _reinitialize(self):\n",
    "        \"\"\"\n",
    "        Tensorflow/Keras-like initialization\n",
    "        \"\"\"\n",
    "        for name, p in self.named_parameters():\n",
    "            if 'rnn' in name:\n",
    "                if 'weight_ih' in name:\n",
    "                    nn.init.xavier_uniform_(p.data)\n",
    "                elif 'weight_hh' in name:\n",
    "                    nn.init.orthogonal_(p.data)\n",
    "                elif 'bias_ih' in name:\n",
    "                    p.data.fill_(0)\n",
    "                    # Set forget-gate bias to 1\n",
    "                    n = p.size(0)\n",
    "                    p.data[(n // 4):(n // 2)].fill_(1)\n",
    "                elif 'bias_hh' in name:\n",
    "                    p.data.fill_(0)\n",
    "    \n",
    "    def forward(self, numerical_array,\n",
    "                mask_array,\n",
    "                attention_mask):\n",
    "        \n",
    "        numerical_embedding = self.numerical_linear(numerical_array)\n",
    "        output,_ = self.rnn(numerical_embedding)\n",
    "        output = self.linear_out(output)\n",
    "        return output\n",
    "    \n",
    "    \n",
    "class DefogRnnModel2(nn.Module):\n",
    "    def __init__(\n",
    "        self, dropout=0.2,\n",
    "        input_numerical_size=9,\n",
    "        numeraical_linear_size = 96,\n",
    "        model_size = 256,\n",
    "        linear_out = 256,\n",
    "        out_size=3):\n",
    "        super(DefogRnnModel2, self).__init__()\n",
    "        self.numerical_linear  = nn.Sequential(\n",
    "                nn.Linear(input_numerical_size, numeraical_linear_size),\n",
    "                nn.LayerNorm(numeraical_linear_size)\n",
    "            )\n",
    "        \n",
    "        self.rnn = nn.GRU(numeraical_linear_size, model_size,\n",
    "                            num_layers = 2, \n",
    "                            batch_first=True,\n",
    "                            bidirectional=True)\n",
    "        self.linear_out  = nn.Sequential(\n",
    "                nn.Linear(model_size*2, \n",
    "                          linear_out),\n",
    "                nn.LayerNorm(linear_out),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(linear_out, \n",
    "                          out_size))\n",
    "        self._reinitialize()\n",
    "        \n",
    "    def _reinitialize(self):\n",
    "        \"\"\"\n",
    "        Tensorflow/Keras-like initialization\n",
    "        \"\"\"\n",
    "        for name, p in self.named_parameters():\n",
    "            if 'rnn' in name:\n",
    "                if 'weight_ih' in name:\n",
    "                    nn.init.xavier_uniform_(p.data)\n",
    "                elif 'weight_hh' in name:\n",
    "                    nn.init.orthogonal_(p.data)\n",
    "                elif 'bias_ih' in name:\n",
    "                    p.data.fill_(0)\n",
    "                    # Set forget-gate bias to 1\n",
    "                    n = p.size(0)\n",
    "                    p.data[(n // 4):(n // 2)].fill_(1)\n",
    "                elif 'bias_hh' in name:\n",
    "                    p.data.fill_(0)\n",
    "    \n",
    "    def forward(self, numerical_array,\n",
    "                mask_array,\n",
    "                attention_mask):\n",
    "        \n",
    "        numerical_embedding = self.numerical_linear(numerical_array)\n",
    "        output,_ = self.rnn(numerical_embedding)\n",
    "        output = self.linear_out(output)\n",
    "        return output\n",
    "    \n",
    "    \n",
    "class Defog3Model(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dropout=0.2,\n",
    "        input_numerical_size=9,\n",
    "        numeraical_linear_size=64,\n",
    "        model_size=128,\n",
    "        linear_out=128,\n",
    "        out_size=3,\n",
    "    ):\n",
    "        super(Defog3Model, self).__init__()\n",
    "        self.numerical_linear = nn.Sequential(\n",
    "                nn.Linear(input_numerical_size, numeraical_linear_size),\n",
    "                nn.LayerNorm(numeraical_linear_size)\n",
    "            )\n",
    "        self.lstm = nn.GRU(\n",
    "            numeraical_linear_size,\n",
    "            model_size,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "        self.linear_out = nn.Sequential(\n",
    "            nn.Linear(model_size*2, linear_out),\n",
    "            nn.LayerNorm(linear_out),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(linear_out, out_size),\n",
    "        )\n",
    "        self._reinitialize()\n",
    "\n",
    "    def _reinitialize(self):\n",
    "        \"\"\"\n",
    "        Tensorflow/Keras-like initialization\n",
    "        \"\"\"\n",
    "        for name, p in self.named_parameters():\n",
    "            if 'lstm' in name:\n",
    "                if 'weight_ih' in name:\n",
    "                    nn.init.xavier_uniform_(p.data)\n",
    "                elif 'weight_hh' in name:\n",
    "                    nn.init.orthogonal_(p.data)\n",
    "                elif 'bias_ih' in name:\n",
    "                    p.data.fill_(0)\n",
    "                    # Set forget-gate bias to 1\n",
    "                    n = p.size(0)\n",
    "                    p.data[(n // 4):(n // 2)].fill_(1)\n",
    "                elif 'bias_hh' in name:\n",
    "                    p.data.fill_(0)\n",
    "\n",
    "    def forward(self, numerical_array, mask_array, attention_mask):\n",
    "        numerical_embedding = self.numerical_linear(numerical_array)\n",
    "        output, _ = self.lstm(numerical_embedding)\n",
    "        output = self.linear_out(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3570eec9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-06T12:45:42.958821Z",
     "iopub.status.busy": "2023-06-06T12:45:42.958172Z",
     "iopub.status.idle": "2023-06-06T12:45:42.969345Z",
     "shell.execute_reply": "2023-06-06T12:45:42.968416Z"
    },
    "papermill": {
     "duration": 0.02161,
     "end_time": "2023-06-06T12:45:42.971626",
     "exception": false,
     "start_time": "2023-06-06T12:45:42.950016",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_pred(test_loader,model):\n",
    "    test_preds = []\n",
    "    tk0 = tqdm(test_loader, total=len(test_loader))\n",
    "    with torch.no_grad():  # Do not calculate gradient since we are only predicting\n",
    "        # Predicting on validation set\n",
    "        for d in tk0:\n",
    "            input_data_numerical_array = d['input_data_numerical_array'].to(device)\n",
    "            input_data_mask_array = d['input_data_mask_array'].to(device)\n",
    "            attention_mask = d['attention_mask'].to(device)\n",
    "            output = model(input_data_numerical_array, \n",
    "                       input_data_mask_array,\n",
    "                       attention_mask)\n",
    "            test_preds.append(output.sigmoid().cpu().numpy())\n",
    "    test_preds = np.concatenate(test_preds,axis=0)\n",
    "    return test_preds\n",
    "\n",
    "\n",
    "def make_pred2(test_loader,model):\n",
    "    test_preds = []\n",
    "    tk0 = tqdm(test_loader, total=len(test_loader))\n",
    "    with torch.no_grad():  # Do not calculate gradient since we are only predicting\n",
    "        # Predicting on validation set\n",
    "        for d in tk0:\n",
    "            input_data_numerical_array = d['input_data_numerical_array'].to(device)\n",
    "            input_data_mask_array = d['input_data_mask_array'].to(device)\n",
    "            attention_mask = d['attention_mask'].to(device)\n",
    "            output,_ = model(input_data_numerical_array, \n",
    "                       input_data_mask_array,\n",
    "                       attention_mask)\n",
    "            test_preds.append(output.sigmoid().cpu().numpy())\n",
    "    test_preds = np.concatenate(test_preds,axis=0)\n",
    "    return test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "697d63b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-06T12:45:42.987868Z",
     "iopub.status.busy": "2023-06-06T12:45:42.987078Z",
     "iopub.status.idle": "2023-06-06T12:45:43.229569Z",
     "shell.execute_reply": "2023-06-06T12:45:43.228482Z"
    },
    "papermill": {
     "duration": 0.253713,
     "end_time": "2023-06-06T12:45:43.232639",
     "exception": false,
     "start_time": "2023-06-06T12:45:42.978926",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =======================\n",
    "# main\n",
    "# =======================\n",
    "sub = pd.read_csv(SUB_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a7a883d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-06T12:45:43.250502Z",
     "iopub.status.busy": "2023-06-06T12:45:43.250149Z",
     "iopub.status.idle": "2023-06-06T12:45:50.947065Z",
     "shell.execute_reply": "2023-06-06T12:45:50.945896Z"
    },
    "papermill": {
     "duration": 7.708417,
     "end_time": "2023-06-06T12:45:50.950038",
     "exception": false,
     "start_time": "2023-06-06T12:45:43.241621",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load weights from /kaggle/input/pd-exp238/fold0_best.pth\n",
      "load weights from /kaggle/input/pd-exp238/fold1_best.pth\n",
      "load weights from /kaggle/input/pd-exp238/fold2_best.pth\n",
      "load weights from /kaggle/input/pd-exp238/fold3_best.pth\n",
      "load weights from /kaggle/input/pd-exp238/fold4_best.pth\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# tdcsfog model\n",
    "# ===========================\n",
    "tdcsfog_model_list1 = []\n",
    "for i in tdcsfog_path1:\n",
    "    model = TdcsfogRnnModel()\n",
    "    model.load_state_dict(torch.load(i))\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    tdcsfog_model_list1.append(model)\n",
    "\n",
    "tdcsfog_model_list3_1 = []\n",
    "for i in tdcsfog_path3_1:\n",
    "    model = TdcsfogRnnModel()\n",
    "    model.load_state_dict(torch.load(i))\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    tdcsfog_model_list3_1.append(model)\n",
    "    \n",
    "tdcsfog_model_list3_2 = []\n",
    "for i in tdcsfog_path3_2:\n",
    "    model = TdcsfogRnnModel()\n",
    "    model.load_state_dict(torch.load(i))\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    tdcsfog_model_list3_2.append(model)\n",
    "    \n",
    "tdcsfog_model_list3_3 = []\n",
    "for i in tdcsfog_path3_3:\n",
    "    model = TdcsfogRnnModel()\n",
    "    model.load_state_dict(torch.load(i))\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    tdcsfog_model_list3_3.append(model)\n",
    "    \n",
    "tdcsfog_model_list4_1 = []\n",
    "for i in tdcsfog_path4_1:\n",
    "    model = TdcsfogRnnModel()\n",
    "    model.load_state_dict(torch.load(i))\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    tdcsfog_model_list4_1.append(model)\n",
    "    \n",
    "tdcsfog_model_list4_2 = []\n",
    "for i in tdcsfog_path4_2:\n",
    "    model = TdcsfogRnnModel()\n",
    "    model.load_state_dict(torch.load(i))\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    tdcsfog_model_list4_2.append(model)\n",
    "    \n",
    "tdcsfog_model_list4_3 = []\n",
    "for i in tdcsfog_path4_3:\n",
    "    model = TdcsfogRnnModel()\n",
    "    model.load_state_dict(torch.load(i))\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    tdcsfog_model_list4_3.append(model)\n",
    "    \n",
    "\n",
    "# ===========================\n",
    "# defog model\n",
    "# ===========================\n",
    "defog_model_list2 = []\n",
    "for i in defog_path2:\n",
    "    model = DefogRnnModel()\n",
    "    model.load_state_dict(torch.load(i))\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    defog_model_list2.append(model)\n",
    "    \n",
    "defog_model_list4 = []\n",
    "for i in defog_path4:\n",
    "    model = DefogRnnModel()\n",
    "    model.load_state_dict(torch.load(i))\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    defog_model_list4.append(model)\n",
    "    \n",
    "defog_model_list5 = []\n",
    "for i in defog_path5:\n",
    "    model = DefogRnnModel()\n",
    "    model.load_state_dict(torch.load(i))\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    defog_model_list5.append(model)\n",
    "    \n",
    "defog_model_list6 = []\n",
    "for i in defog_path6:\n",
    "    model = DefogRnnModel2()\n",
    "    model.load_state_dict(torch.load(i))\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    defog_model_list6.append(model)\n",
    "\n",
    "defog_model_list7 = []\n",
    "for path in defog_path7:\n",
    "    model = Defog3Model()\n",
    "    state = torch.load(path, map_location=torch.device(\"cpu\"))\n",
    "    model.load_state_dict(state[\"model\"])\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    defog_model_list7.append(model)\n",
    "    print(f\"load weights from {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac01e727",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-06T12:45:50.967834Z",
     "iopub.status.busy": "2023-06-06T12:45:50.966923Z",
     "iopub.status.idle": "2023-06-06T12:45:50.972024Z",
     "shell.execute_reply": "2023-06-06T12:45:50.970966Z"
    },
    "papermill": {
     "duration": 0.01619,
     "end_time": "2023-06-06T12:45:50.974375",
     "exception": false,
     "start_time": "2023-06-06T12:45:50.958185",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_all = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5af885f",
   "metadata": {
    "papermill": {
     "duration": 0.007403,
     "end_time": "2023-06-06T12:45:50.989187",
     "exception": false,
     "start_time": "2023-06-06T12:45:50.981784",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# tdcsfog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40bb4bd3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-06T12:45:51.005667Z",
     "iopub.status.busy": "2023-06-06T12:45:51.005343Z",
     "iopub.status.idle": "2023-06-06T12:45:53.273376Z",
     "shell.execute_reply": "2023-06-06T12:45:53.272042Z"
    },
    "papermill": {
     "duration": 2.279225,
     "end_time": "2023-06-06T12:45:53.275915",
     "exception": false,
     "start_time": "2023-06-06T12:45:50.996690",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.76s/it]\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 10.65it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 1/1 [00:00<00:00,  9.04it/s]\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 10.90it/s]\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 10.92it/s]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.23s/it]\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# tdcsfog1\n",
    "# =========================\n",
    "th_len = 5000\n",
    "w = 0.20\n",
    "\n",
    "tdcsfog_list = glob.glob(TDCSFOG_DATA_PATH)\n",
    "cols = [\"AccV\",\"AccML\",\"AccAP\"]\n",
    "num_cols = ['AccV', 'AccML', 'AccAP', \n",
    "       'AccV_lag_diff', 'AccV_lead_diff', 'AccV_cumsum', 'AccML_lag_diff',\n",
    "       'AccML_lead_diff', 'AccML_cumsum', 'AccAP_lag_diff', 'AccAP_lead_diff',\n",
    "       'AccAP_cumsum']\n",
    "for p in tqdm(tdcsfog_list):\n",
    "    id_values = p.split(\"/\")[-1].split(\".\")[0]\n",
    "    df = pd.read_csv(p)\n",
    "    \n",
    "    if len(df) > th_len:\n",
    "        seq_len = 5000\n",
    "        shift = 2500\n",
    "        offset = 1250\n",
    "    else:\n",
    "        seq_len = 3000\n",
    "        shift = 1500\n",
    "        offset = 750\n",
    "        \n",
    "    batch = (len(df)-1) // shift\n",
    "    if batch == 0:\n",
    "        batch = 1\n",
    "    for c in cols:\n",
    "        df[f\"{c}_lag_diff\"] = df[c].diff()\n",
    "        df[f\"{c}_lead_diff\"] = df[c].diff(-1)\n",
    "        df[f\"{c}_cumsum\"] = df[c].cumsum()\n",
    "    sc = RobustScaler()\n",
    "    df[num_cols] = sc.fit_transform(df[num_cols].values)\n",
    "    df[num_cols] = df[num_cols].fillna(0)\n",
    "    num = df[num_cols].values\n",
    "    time = df[\"Time\"].values\n",
    "    \n",
    "    num_array = np.zeros([batch,seq_len,12])\n",
    "    mask_array = np.zeros([batch,seq_len],dtype=int)\n",
    "    time_array = np.zeros([batch,seq_len],dtype=int)\n",
    "    pred_use_array = np.zeros([batch,seq_len],dtype=int)\n",
    "    \n",
    "    if len(df) <= seq_len:\n",
    "        b = 0\n",
    "        num_ = num.copy()\n",
    "        time_ = time.copy()\n",
    "        num_len = len(num_)\n",
    "\n",
    "        num_array[b,:num_len,:] = num_\n",
    "        time_array[b,:num_len] = time_\n",
    "        mask_array[b,:num_len] = 1\n",
    "        pred_use_array[b,:num_len] = 1\n",
    "    else:\n",
    "        for n,b in enumerate(range(batch)):\n",
    "            if b == (batch - 1):\n",
    "                num_ = num[b*shift : ]\n",
    "                time_ = time[b*shift : ]\n",
    "                num_len = len(num_)\n",
    "\n",
    "                num_array[b,:num_len,:] = num_\n",
    "                time_array[b,:num_len] = time_\n",
    "                mask_array[b,:num_len] = 1\n",
    "                pred_use_array[b,offset:num_len] = 1\n",
    "            elif b == 0:\n",
    "                num_ = num[b*shift:b*shift+seq_len]\n",
    "                time_ = time[b*shift:b*shift + seq_len]\n",
    "\n",
    "                num_array[b,:,:] = num_\n",
    "                time_array[b,:] = time_\n",
    "                mask_array[b,:] = 1\n",
    "                pred_use_array[b,:shift+offset] = 1\n",
    "            else:\n",
    "                num_ = num[b*shift:b*shift+seq_len]\n",
    "                time_ = time[b*shift:b*shift + seq_len]\n",
    "\n",
    "                num_array[b,:,:] = num_\n",
    "                time_array[b,:] = time_\n",
    "                mask_array[b,:] = 1\n",
    "                pred_use_array[b,offset:shift+offset] = 1\n",
    "            \n",
    "            \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    test_ = FogDataset(num_array,\n",
    "                       mask_array,\n",
    "                       train=False)\n",
    "    test_loader = DataLoader(dataset=test_, \n",
    "                        batch_size=bs, \n",
    "                        shuffle = False)\n",
    "    for n,m in enumerate(tdcsfog_model_list1):\n",
    "        if n == 0:\n",
    "            pred = make_pred(test_loader,m) / len(tdcsfog_model_list1)\n",
    "        else:\n",
    "            pred += make_pred(test_loader,m) / len(tdcsfog_model_list1)\n",
    "    pred_list = []\n",
    "    for i in range(batch):\n",
    "        mask_ = pred_use_array[i]\n",
    "        pred_ = pred[i,mask_ == 1,:]\n",
    "        time_ = time_array[i, mask_ == 1]\n",
    "        df_ = pd.DataFrame()\n",
    "        df_[\"StartHesitation\"] = pred_[:,0] * w\n",
    "        df_[\"Turn\"] = pred_[:,1] * w\n",
    "        df_[\"Walking\"] = pred_[:,2] * w\n",
    "        df_[\"Time\"] = time_\n",
    "        df_[\"Id\"] = id_values\n",
    "        df_[\"Id\"] = df_[\"Id\"].astype(str) + \"_\" + df_[\"Time\"].astype(str)\n",
    "        pred_list.append(df_)\n",
    "    pred = pd.concat(pred_list).reset_index(drop=True)\n",
    "    df_all.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9edcc3b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-06T12:45:53.299019Z",
     "iopub.status.busy": "2023-06-06T12:45:53.298294Z",
     "iopub.status.idle": "2023-06-06T12:45:54.973356Z",
     "shell.execute_reply": "2023-06-06T12:45:54.972145Z"
    },
    "papermill": {
     "duration": 1.690604,
     "end_time": "2023-06-06T12:45:54.977205",
     "exception": false,
     "start_time": "2023-06-06T12:45:53.286601",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10.87it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 1/1 [00:00<00:00,  9.04it/s]\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 10.92it/s]\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 10.71it/s]\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 10.94it/s]\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 10.94it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.97it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.83it/s]\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 10.81it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.97it/s]\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 10.59it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 1/1 [00:00<00:00,  9.17it/s]\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 10.21it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.99it/s]\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 10.89it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.64s/it]\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# tdcsfog3\n",
    "# =========================\n",
    "th_len = 5000\n",
    "w = 0.40\n",
    "\n",
    "tdcsfog_list = glob.glob(TDCSFOG_DATA_PATH)\n",
    "cols = [\"AccV\",\"AccML\",\"AccAP\"]\n",
    "num_cols = ['AccV', 'AccML', 'AccAP', \n",
    "       'AccV_lag_diff', 'AccV_lead_diff', 'AccV_cumsum', 'AccML_lag_diff',\n",
    "       'AccML_lead_diff', 'AccML_cumsum', 'AccAP_lag_diff', 'AccAP_lead_diff',\n",
    "       'AccAP_cumsum']\n",
    "for p in tqdm(tdcsfog_list):\n",
    "    id_values = p.split(\"/\")[-1].split(\".\")[0]\n",
    "    df = pd.read_csv(p)\n",
    "    if len(df) > th_len:\n",
    "        seq_len = 5000\n",
    "        shift = 2500\n",
    "        offset = 1250\n",
    "    else:\n",
    "        seq_len = 3000\n",
    "        shift = 1500\n",
    "        offset = 750\n",
    "    batch = (len(df)-1) // shift\n",
    "    if batch == 0:\n",
    "        batch = 1\n",
    "    for c in cols:\n",
    "        df[f\"{c}_lag_diff\"] = df[c].diff()\n",
    "        df[f\"{c}_lead_diff\"] = df[c].diff(-1)\n",
    "        df[f\"{c}_cumsum\"] = df[c].cumsum()\n",
    "    sc = RobustScaler()\n",
    "    df[num_cols] = sc.fit_transform(df[num_cols].values)\n",
    "    df[num_cols] = df[num_cols].fillna(0)\n",
    "    num = df[num_cols].values\n",
    "    time = df[\"Time\"].values\n",
    "    \n",
    "    num_array = np.zeros([batch,seq_len,12])\n",
    "    mask_array = np.zeros([batch,seq_len],dtype=int)\n",
    "    time_array = np.zeros([batch,seq_len],dtype=int)\n",
    "    pred_use_array = np.zeros([batch,seq_len],dtype=int)\n",
    "    \n",
    "    if len(df) <= seq_len:\n",
    "        b = 0\n",
    "        num_ = num.copy()\n",
    "        time_ = time.copy()\n",
    "        num_len = len(num_)\n",
    "\n",
    "        num_array[b,:num_len,:] = num_\n",
    "        time_array[b,:num_len] = time_\n",
    "        mask_array[b,:num_len] = 1\n",
    "        pred_use_array[b,:num_len] = 1\n",
    "    else:\n",
    "        for n,b in enumerate(range(batch)):\n",
    "            if b == (batch - 1):\n",
    "                num_ = num[b*shift : ]\n",
    "                time_ = time[b*shift : ]\n",
    "                num_len = len(num_)\n",
    "\n",
    "                num_array[b,:num_len,:] = num_\n",
    "                time_array[b,:num_len] = time_\n",
    "                mask_array[b,:num_len] = 1\n",
    "                pred_use_array[b,offset:num_len] = 1\n",
    "            elif b == 0:\n",
    "                num_ = num[b*shift:b*shift+seq_len]\n",
    "                time_ = time[b*shift:b*shift + seq_len]\n",
    "\n",
    "                num_array[b,:,:] = num_\n",
    "                time_array[b,:] = time_\n",
    "                mask_array[b,:] = 1\n",
    "                pred_use_array[b,:shift+offset] = 1\n",
    "            else:\n",
    "                num_ = num[b*shift:b*shift+seq_len]\n",
    "                time_ = time[b*shift:b*shift + seq_len]\n",
    "\n",
    "                num_array[b,:,:] = num_\n",
    "                time_array[b,:] = time_\n",
    "                mask_array[b,:] = 1\n",
    "                pred_use_array[b,offset:shift+offset] = 1\n",
    "    \n",
    "    \n",
    "    \n",
    "    test_ = FogDataset(num_array,\n",
    "                       mask_array,\n",
    "                       train=False)\n",
    "    test_loader = DataLoader(dataset=test_, \n",
    "                        batch_size=bs, \n",
    "                        shuffle = False)\n",
    "    for n,m in enumerate(tdcsfog_model_list3_1):\n",
    "        if n == 0:\n",
    "            pred1 = make_pred(test_loader,m) / len(tdcsfog_model_list3_1)\n",
    "        else:\n",
    "            pred1 += make_pred(test_loader,m) / len(tdcsfog_model_list3_1)\n",
    "    for n,m in enumerate(tdcsfog_model_list3_2):\n",
    "        if n == 0:\n",
    "            pred2 = make_pred(test_loader,m) / len(tdcsfog_model_list3_2)\n",
    "        else:\n",
    "            pred2 += make_pred(test_loader,m) / len(tdcsfog_model_list3_2)\n",
    "    for n,m in enumerate(tdcsfog_model_list3_3):\n",
    "        if n == 0:\n",
    "            pred3 = make_pred(test_loader,m) / len(tdcsfog_model_list3_3)\n",
    "        else:\n",
    "            pred3 += make_pred(test_loader,m) / len(tdcsfog_model_list3_3)\n",
    "    pred = pred1.copy()\n",
    "    pred[:,:,1] = pred2[:,:,1]\n",
    "    pred[:,:,2] = pred3[:,:,2]\n",
    "    \n",
    "    \n",
    "    pred_list = []\n",
    "    for i in range(batch):\n",
    "        mask_ = pred_use_array[i]\n",
    "        pred_ = pred[i,mask_ == 1,:]\n",
    "        time_ = time_array[i, mask_ == 1]\n",
    "        df_ = pd.DataFrame()\n",
    "        df_[\"StartHesitation\"] = pred_[:,0] * w\n",
    "        df_[\"Turn\"] = pred_[:,1] * w\n",
    "        df_[\"Walking\"] = pred_[:,2] * w\n",
    "        df_[\"Time\"] = time_\n",
    "        df_[\"Id\"] = id_values\n",
    "        df_[\"Id\"] = df_[\"Id\"].astype(str) + \"_\" + df_[\"Time\"].astype(str)\n",
    "        pred_list.append(df_)\n",
    "    pred = pd.concat(pred_list).reset_index(drop=True)\n",
    "    df_all.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43c64fd2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-06T12:45:55.013485Z",
     "iopub.status.busy": "2023-06-06T12:45:55.013146Z",
     "iopub.status.idle": "2023-06-06T12:45:56.713026Z",
     "shell.execute_reply": "2023-06-06T12:45:56.711009Z"
    },
    "papermill": {
     "duration": 1.721,
     "end_time": "2023-06-06T12:45:56.715707",
     "exception": false,
     "start_time": "2023-06-06T12:45:54.994707",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 1/1 [00:00<00:00,  9.03it/s]\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 10.82it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 1/1 [00:00<00:00,  9.06it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.94it/s]\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 10.73it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 1/1 [00:00<00:00,  9.07it/s]\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 10.76it/s]\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 10.63it/s]\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 10.87it/s]\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 10.56it/s]\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 10.69it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.90it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.91it/s]\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 10.77it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.98it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.66s/it]\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# tdcsfog4\n",
    "# =========================\n",
    "th_len = 5000\n",
    "w = 0.40\n",
    "\n",
    "tdcsfog_list = glob.glob(TDCSFOG_DATA_PATH)\n",
    "cols = [\"AccV\",\"AccML\",\"AccAP\"]\n",
    "num_cols = ['AccV', 'AccML', 'AccAP', \n",
    "       'AccV_lag_diff', 'AccV_lead_diff', 'AccV_cumsum', 'AccML_lag_diff',\n",
    "       'AccML_lead_diff', 'AccML_cumsum', 'AccAP_lag_diff', 'AccAP_lead_diff',\n",
    "       'AccAP_cumsum']\n",
    "for p in tqdm(tdcsfog_list):\n",
    "    id_values = p.split(\"/\")[-1].split(\".\")[0]\n",
    "    df = pd.read_csv(p)\n",
    "    if len(df) > th_len:\n",
    "        seq_len = 5000\n",
    "        shift = 2500\n",
    "        offset = 1250\n",
    "    else:\n",
    "        seq_len = 3000\n",
    "        shift = 1500\n",
    "        offset = 750\n",
    "    batch = (len(df)-1) // shift\n",
    "    if batch == 0:\n",
    "        batch = 1\n",
    "    for c in cols:\n",
    "        df[f\"{c}_lag_diff\"] = df[c].diff()\n",
    "        df[f\"{c}_lead_diff\"] = df[c].diff(-1)\n",
    "        df[f\"{c}_cumsum\"] = df[c].cumsum()\n",
    "    sc = RobustScaler()\n",
    "    df[num_cols] = sc.fit_transform(df[num_cols].values)\n",
    "    df[num_cols] = df[num_cols].fillna(0)\n",
    "    num = df[num_cols].values\n",
    "    time = df[\"Time\"].values\n",
    "    \n",
    "    num_array = np.zeros([batch,seq_len,12])\n",
    "    mask_array = np.zeros([batch,seq_len],dtype=int)\n",
    "    time_array = np.zeros([batch,seq_len],dtype=int)\n",
    "    pred_use_array = np.zeros([batch,seq_len],dtype=int)\n",
    "    \n",
    "    if len(df) <= seq_len:\n",
    "        b = 0\n",
    "        num_ = num.copy()\n",
    "        time_ = time.copy()\n",
    "        num_len = len(num_)\n",
    "\n",
    "        num_array[b,:num_len,:] = num_\n",
    "        time_array[b,:num_len] = time_\n",
    "        mask_array[b,:num_len] = 1\n",
    "        pred_use_array[b,:num_len] = 1\n",
    "    else:\n",
    "        for n,b in enumerate(range(batch)):\n",
    "            if b == (batch - 1):\n",
    "                num_ = num[b*shift : ]\n",
    "                time_ = time[b*shift : ]\n",
    "                num_len = len(num_)\n",
    "\n",
    "                num_array[b,:num_len,:] = num_\n",
    "                time_array[b,:num_len] = time_\n",
    "                mask_array[b,:num_len] = 1\n",
    "                pred_use_array[b,offset:num_len] = 1\n",
    "            elif b == 0:\n",
    "                num_ = num[b*shift:b*shift+seq_len]\n",
    "                time_ = time[b*shift:b*shift + seq_len]\n",
    "\n",
    "                num_array[b,:,:] = num_\n",
    "                time_array[b,:] = time_\n",
    "                mask_array[b,:] = 1\n",
    "                pred_use_array[b,:shift+offset] = 1\n",
    "            else:\n",
    "                num_ = num[b*shift:b*shift+seq_len]\n",
    "                time_ = time[b*shift:b*shift + seq_len]\n",
    "\n",
    "                num_array[b,:,:] = num_\n",
    "                time_array[b,:] = time_\n",
    "                mask_array[b,:] = 1\n",
    "                pred_use_array[b,offset:shift+offset] = 1\n",
    "    \n",
    "    \n",
    "    \n",
    "    test_ = FogDataset(num_array,\n",
    "                       mask_array,\n",
    "                       train=False)\n",
    "    test_loader = DataLoader(dataset=test_, \n",
    "                        batch_size=bs, \n",
    "                        shuffle = False)\n",
    "    for n,m in enumerate(tdcsfog_model_list4_1):\n",
    "        if n == 0:\n",
    "            pred1 = make_pred(test_loader,m) / len(tdcsfog_model_list4_1)\n",
    "        else:\n",
    "            pred1 += make_pred(test_loader,m) / len(tdcsfog_model_list4_1)\n",
    "    for n,m in enumerate(tdcsfog_model_list4_2):\n",
    "        if n == 0:\n",
    "            pred2 = make_pred(test_loader,m) / len(tdcsfog_model_list4_2)\n",
    "        else:\n",
    "            pred2 += make_pred(test_loader,m) / len(tdcsfog_model_list4_2)\n",
    "    for n,m in enumerate(tdcsfog_model_list4_3):\n",
    "        if n == 0:\n",
    "            pred3 = make_pred(test_loader,m) / len(tdcsfog_model_list4_3)\n",
    "        else:\n",
    "            pred3 += make_pred(test_loader,m) / len(tdcsfog_model_list4_3)\n",
    "    pred = pred1.copy()\n",
    "    pred[:,:,0] = pred1[:,:,0]*0.5 + pred2[:,:,0]*0.5\n",
    "    pred[:,:,1] = pred1[:,:,1]*0.5 + pred3[:,:,1]*0.5\n",
    "    pred[:,:,2] = pred2[:,:,2]*0.5 + pred3[:,:,2]*0.5\n",
    "    \n",
    "    \n",
    "    pred_list = []\n",
    "    for i in range(batch):\n",
    "        mask_ = pred_use_array[i]\n",
    "        pred_ = pred[i,mask_ == 1,:]\n",
    "        time_ = time_array[i, mask_ == 1]\n",
    "        df_ = pd.DataFrame()\n",
    "        df_[\"StartHesitation\"] = pred_[:,0] * w\n",
    "        df_[\"Turn\"] = pred_[:,1] * w\n",
    "        df_[\"Walking\"] = pred_[:,2] * w\n",
    "        df_[\"Time\"] = time_\n",
    "        df_[\"Id\"] = id_values\n",
    "        df_[\"Id\"] = df_[\"Id\"].astype(str) + \"_\" + df_[\"Time\"].astype(str)\n",
    "        pred_list.append(df_)\n",
    "    pred = pd.concat(pred_list).reset_index(drop=True)\n",
    "    df_all.append(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52412f0",
   "metadata": {
    "papermill": {
     "duration": 0.024176,
     "end_time": "2023-06-06T12:45:56.765745",
     "exception": false,
     "start_time": "2023-06-06T12:45:56.741569",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# defog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3d2b533",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-06T12:45:56.816159Z",
     "iopub.status.busy": "2023-06-06T12:45:56.815838Z",
     "iopub.status.idle": "2023-06-06T12:46:04.405581Z",
     "shell.execute_reply": "2023-06-06T12:46:04.404358Z"
    },
    "papermill": {
     "duration": 7.620635,
     "end_time": "2023-06-06T12:46:04.410780",
     "exception": false,
     "start_time": "2023-06-06T12:45:56.790145",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.32s/it]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.35s/it]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.30s/it]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.31s/it]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.30s/it]\n",
      "100%|██████████| 1/1 [00:07<00:00,  7.55s/it]\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# defog2\n",
    "# =========================\n",
    "th_len = 200000\n",
    "w = 0.35\n",
    "\n",
    "defog_list = glob.glob(DEFOG_DATA_PATH)\n",
    "cols = [\"AccV\",\"AccML\",\"AccAP\"]\n",
    "num_cols = [\"AccV\",\"AccML\",\"AccAP\",'AccV_lag_diff',\n",
    "            'AccV_lead_diff', 'AccML_lag_diff', 'AccML_lead_diff',\n",
    "            'AccAP_lag_diff', 'AccAP_lead_diff']\n",
    "for p in tqdm(defog_list):\n",
    "    id_values = p.split(\"/\")[-1].split(\".\")[0]\n",
    "    df = pd.read_csv(p)\n",
    "    if len(df) > th_len:\n",
    "        seq_len = 30000\n",
    "        shift = 15000\n",
    "        offset = 7500\n",
    "    else:\n",
    "        seq_len = 15000\n",
    "        shift = 7500\n",
    "        offset = 3750\n",
    "    batch = (len(df)-1) // shift\n",
    "    if batch == 0:\n",
    "        batch = 1\n",
    "    for c in cols:\n",
    "        df[f\"{c}_lag_diff\"] = df[c].diff()\n",
    "        df[f\"{c}_lead_diff\"] = df[c].diff(-1)\n",
    "    sc = StandardScaler()\n",
    "    df[num_cols] = sc.fit_transform(df[num_cols].values)\n",
    "    df[num_cols] = df[num_cols].fillna(0)\n",
    "    num = df[num_cols].values\n",
    "    time = df[\"Time\"].values\n",
    "    \n",
    "    num_array = np.zeros([batch,seq_len,9])\n",
    "    mask_array = np.zeros([batch,seq_len],dtype=int)\n",
    "    time_array = np.zeros([batch,seq_len],dtype=int)\n",
    "    pred_use_array = np.zeros([batch,seq_len],dtype=int)\n",
    "    \n",
    "    if len(df) <= seq_len:\n",
    "        b = 0\n",
    "        num_len = len(num)\n",
    "        num_array[b,:num_len,:] = num\n",
    "        time_array[b,:num_len] = time\n",
    "        mask_array[b,:num_len] = 1\n",
    "        pred_use_array[b,:num_len] = 1\n",
    "    else:\n",
    "        for n,b in enumerate(range(batch)):\n",
    "            if b == (batch - 1):\n",
    "                num_ = num[b*shift : ]\n",
    "                time_ = time[b*shift : ]\n",
    "                num_len = len(num_)\n",
    "\n",
    "                num_array[b,:num_len,:] = num_\n",
    "                time_array[b,:num_len] = time_\n",
    "                mask_array[b,:num_len] = 1\n",
    "                pred_use_array[b,offset:num_len] = 1\n",
    "            elif b == 0:\n",
    "                num_ = num[b*shift:b*shift+seq_len]\n",
    "                time_ = time[b*shift:b*shift + seq_len]\n",
    "\n",
    "                num_array[b,:,:] = num_\n",
    "                time_array[b,:] = time_\n",
    "                mask_array[b,:] = 1\n",
    "                pred_use_array[b,:shift+offset] = 1\n",
    "            else:\n",
    "                num_ = num[b*shift:b*shift+seq_len]\n",
    "                time_ = time[b*shift:b*shift + seq_len]\n",
    "\n",
    "                num_array[b,:,:] = num_\n",
    "                time_array[b,:] = time_\n",
    "                mask_array[b,:] = 1\n",
    "                pred_use_array[b,offset:shift+offset] = 1  \n",
    "    \n",
    "    test_ = FogDataset(num_array,\n",
    "                       mask_array,\n",
    "                       train=False)\n",
    "    test_loader = DataLoader(dataset=test_, \n",
    "                        batch_size=bs, \n",
    "                        shuffle = False)\n",
    "    for n,m in enumerate(defog_model_list2):\n",
    "        if n == 0:\n",
    "            pred = make_pred(test_loader,m) / len(defog_model_list2)\n",
    "        else:\n",
    "            pred += make_pred(test_loader,m) / len(defog_model_list2)\n",
    "    pred_list = []\n",
    "    for i in range(batch):\n",
    "        mask_ = pred_use_array[i]\n",
    "        pred_ = pred[i,mask_ == 1,:]\n",
    "        time_ = time_array[i, mask_ == 1]\n",
    "        df_ = pd.DataFrame()\n",
    "        df_[\"StartHesitation\"] = pred_[:,0] * w\n",
    "        df_[\"Turn\"] = pred_[:,1] * w\n",
    "        df_[\"Walking\"] = pred_[:,2] * w\n",
    "        df_[\"Time\"] = time_\n",
    "        df_[\"Id\"] = id_values\n",
    "        df_[\"Id\"] = df_[\"Id\"].astype(str) + \"_\" + df_[\"Time\"].astype(str)\n",
    "        pred_list.append(df_)\n",
    "    pred = pd.concat(pred_list).reset_index(drop=True)\n",
    "    df_all.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c8c0f82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-06T12:46:04.468821Z",
     "iopub.status.busy": "2023-06-06T12:46:04.468484Z",
     "iopub.status.idle": "2023-06-06T12:46:11.812897Z",
     "shell.execute_reply": "2023-06-06T12:46:11.811721Z"
    },
    "papermill": {
     "duration": 7.379123,
     "end_time": "2023-06-06T12:46:11.818094",
     "exception": false,
     "start_time": "2023-06-06T12:46:04.438971",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.31s/it]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.29s/it]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.31s/it]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.29s/it]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.34s/it]\n",
      "100%|██████████| 1/1 [00:07<00:00,  7.31s/it]\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# defog4\n",
    "# =========================\n",
    "th_len = 200000\n",
    "w = 0.25\n",
    "\n",
    "defog_list = glob.glob(DEFOG_DATA_PATH)\n",
    "cols = [\"AccV\",\"AccML\",\"AccAP\"]\n",
    "num_cols = [\"AccV\",\"AccML\",\"AccAP\",'AccV_lag_diff',\n",
    "            'AccV_lead_diff', 'AccML_lag_diff', 'AccML_lead_diff',\n",
    "            'AccAP_lag_diff', 'AccAP_lead_diff']\n",
    "for p in tqdm(defog_list):\n",
    "    id_values = p.split(\"/\")[-1].split(\".\")[0]\n",
    "    df = pd.read_csv(p)\n",
    "    if len(df) > th_len:\n",
    "        seq_len = 30000\n",
    "        shift = 15000\n",
    "        offset = 7500\n",
    "    else:\n",
    "        seq_len = 15000\n",
    "        shift = 7500\n",
    "        offset = 3750\n",
    "    batch = (len(df)-1) // shift\n",
    "    if batch == 0:\n",
    "        batch = 1\n",
    "    for c in cols:\n",
    "        df[f\"{c}_lag_diff\"] = df[c].diff()\n",
    "        df[f\"{c}_lead_diff\"] = df[c].diff(-1)\n",
    "    sc = StandardScaler()\n",
    "    df[num_cols] = sc.fit_transform(df[num_cols].values)\n",
    "    df[num_cols] = df[num_cols].fillna(0)\n",
    "    num = df[num_cols].values\n",
    "    time = df[\"Time\"].values\n",
    "    \n",
    "    num_array = np.zeros([batch,seq_len,9])\n",
    "    mask_array = np.zeros([batch,seq_len],dtype=int)\n",
    "    time_array = np.zeros([batch,seq_len],dtype=int)\n",
    "    pred_use_array = np.zeros([batch,seq_len],dtype=int)\n",
    "    \n",
    "    if len(df) <= seq_len:\n",
    "        b = 0\n",
    "        num_len = len(num)\n",
    "        num_array[b,:num_len,:] = num\n",
    "        time_array[b,:num_len] = time\n",
    "        mask_array[b,:num_len] = 1\n",
    "        pred_use_array[b,:num_len] = 1\n",
    "    else:\n",
    "        for n,b in enumerate(range(batch)):\n",
    "            if b == (batch - 1):\n",
    "                num_ = num[b*shift : ]\n",
    "                time_ = time[b*shift : ]\n",
    "                num_len = len(num_)\n",
    "\n",
    "                num_array[b,:num_len,:] = num_\n",
    "                time_array[b,:num_len] = time_\n",
    "                mask_array[b,:num_len] = 1\n",
    "                pred_use_array[b,offset:num_len] = 1\n",
    "            elif b == 0:\n",
    "                num_ = num[b*shift:b*shift+seq_len]\n",
    "                time_ = time[b*shift:b*shift + seq_len]\n",
    "\n",
    "                num_array[b,:,:] = num_\n",
    "                time_array[b,:] = time_\n",
    "                mask_array[b,:] = 1\n",
    "                pred_use_array[b,:shift+offset] = 1\n",
    "            else:\n",
    "                num_ = num[b*shift:b*shift+seq_len]\n",
    "                time_ = time[b*shift:b*shift + seq_len]\n",
    "\n",
    "                num_array[b,:,:] = num_\n",
    "                time_array[b,:] = time_\n",
    "                mask_array[b,:] = 1\n",
    "                pred_use_array[b,offset:shift+offset] = 1  \n",
    "    \n",
    "    test_ = FogDataset(num_array,\n",
    "                       mask_array,\n",
    "                       train=False)\n",
    "    test_loader = DataLoader(dataset=test_, \n",
    "                        batch_size=bs, \n",
    "                        shuffle = False)\n",
    "    for n,m in enumerate(defog_model_list4):\n",
    "        if n == 0:\n",
    "            pred = make_pred(test_loader,m) / len(defog_model_list4)\n",
    "        else:\n",
    "            pred += make_pred(test_loader,m) / len(defog_model_list4)\n",
    "    pred_list = []\n",
    "    for i in range(batch):\n",
    "        mask_ = pred_use_array[i]\n",
    "        pred_ = pred[i,mask_ == 1,:]\n",
    "        time_ = time_array[i, mask_ == 1]\n",
    "        df_ = pd.DataFrame()\n",
    "        df_[\"StartHesitation\"] = pred_[:,0] * w\n",
    "        df_[\"Turn\"] = pred_[:,1] * w\n",
    "        df_[\"Walking\"] = pred_[:,2] * w\n",
    "        df_[\"Time\"] = time_\n",
    "        df_[\"Id\"] = id_values\n",
    "        df_[\"Id\"] = df_[\"Id\"].astype(str) + \"_\" + df_[\"Time\"].astype(str)\n",
    "        pred_list.append(df_)\n",
    "    pred = pd.concat(pred_list).reset_index(drop=True)\n",
    "    df_all.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c4e39048",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-06T12:46:11.880899Z",
     "iopub.status.busy": "2023-06-06T12:46:11.880587Z",
     "iopub.status.idle": "2023-06-06T12:46:19.338228Z",
     "shell.execute_reply": "2023-06-06T12:46:19.337010Z"
    },
    "papermill": {
     "duration": 7.494836,
     "end_time": "2023-06-06T12:46:19.343614",
     "exception": false,
     "start_time": "2023-06-06T12:46:11.848778",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.29s/it]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.33s/it]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.45s/it]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.30s/it]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.30s/it]\n",
      "100%|██████████| 1/1 [00:07<00:00,  7.43s/it]\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# defog5\n",
    "# =========================\n",
    "th_len = 200000\n",
    "w = 0.25\n",
    "\n",
    "defog_list = glob.glob(DEFOG_DATA_PATH)\n",
    "cols = [\"AccV\",\"AccML\",\"AccAP\"]\n",
    "num_cols = [\"AccV\",\"AccML\",\"AccAP\",'AccV_lag_diff',\n",
    "            'AccV_lead_diff', 'AccML_lag_diff', 'AccML_lead_diff',\n",
    "            'AccAP_lag_diff', 'AccAP_lead_diff']\n",
    "for p in tqdm(defog_list):\n",
    "    id_values = p.split(\"/\")[-1].split(\".\")[0]\n",
    "    df = pd.read_csv(p)\n",
    "    if len(df) > th_len:\n",
    "        seq_len = 30000\n",
    "        shift = 15000\n",
    "        offset = 7500\n",
    "    else:\n",
    "        seq_len = 15000\n",
    "        shift = 7500\n",
    "        offset = 3750\n",
    "    batch = (len(df)-1) // shift\n",
    "    if batch == 0:\n",
    "        batch = 1\n",
    "    for c in cols:\n",
    "        df[f\"{c}_lag_diff\"] = df[c].diff()\n",
    "        df[f\"{c}_lead_diff\"] = df[c].diff(-1)\n",
    "    sc = StandardScaler()\n",
    "    df[num_cols] = sc.fit_transform(df[num_cols].values)\n",
    "    df[num_cols] = df[num_cols].fillna(0)\n",
    "    num = df[num_cols].values\n",
    "    time = df[\"Time\"].values\n",
    "    \n",
    "    num_array = np.zeros([batch,seq_len,9])\n",
    "    mask_array = np.zeros([batch,seq_len],dtype=int)\n",
    "    time_array = np.zeros([batch,seq_len],dtype=int)\n",
    "    pred_use_array = np.zeros([batch,seq_len],dtype=int)\n",
    "    \n",
    "    if len(df) <= seq_len:\n",
    "        b = 0\n",
    "        num_len = len(num)\n",
    "        num_array[b,:num_len,:] = num\n",
    "        time_array[b,:num_len] = time\n",
    "        mask_array[b,:num_len] = 1\n",
    "        pred_use_array[b,:num_len] = 1\n",
    "    else:\n",
    "        for n,b in enumerate(range(batch)):\n",
    "            if b == (batch - 1):\n",
    "                num_ = num[b*shift : ]\n",
    "                time_ = time[b*shift : ]\n",
    "                num_len = len(num_)\n",
    "\n",
    "                num_array[b,:num_len,:] = num_\n",
    "                time_array[b,:num_len] = time_\n",
    "                mask_array[b,:num_len] = 1\n",
    "                pred_use_array[b,offset:num_len] = 1\n",
    "            elif b == 0:\n",
    "                num_ = num[b*shift:b*shift+seq_len]\n",
    "                time_ = time[b*shift:b*shift + seq_len]\n",
    "\n",
    "                num_array[b,:,:] = num_\n",
    "                time_array[b,:] = time_\n",
    "                mask_array[b,:] = 1\n",
    "                pred_use_array[b,:shift+offset] = 1\n",
    "            else:\n",
    "                num_ = num[b*shift:b*shift+seq_len]\n",
    "                time_ = time[b*shift:b*shift + seq_len]\n",
    "\n",
    "                num_array[b,:,:] = num_\n",
    "                time_array[b,:] = time_\n",
    "                mask_array[b,:] = 1\n",
    "                pred_use_array[b,offset:shift+offset] = 1  \n",
    "    \n",
    "    test_ = FogDataset(num_array,\n",
    "                       mask_array,\n",
    "                       train=False)\n",
    "    test_loader = DataLoader(dataset=test_, \n",
    "                        batch_size=bs, \n",
    "                        shuffle = False)\n",
    "    for n,m in enumerate(defog_model_list5):\n",
    "        if n == 0:\n",
    "            pred = make_pred(test_loader,m) / len(defog_model_list5)\n",
    "        else:\n",
    "            pred += make_pred(test_loader,m) / len(defog_model_list5)\n",
    "    pred_list = []\n",
    "    for i in range(batch):\n",
    "        mask_ = pred_use_array[i]\n",
    "        pred_ = pred[i,mask_ == 1,:]\n",
    "        time_ = time_array[i, mask_ == 1]\n",
    "        df_ = pd.DataFrame()\n",
    "        df_[\"StartHesitation\"] = pred_[:,0] * w\n",
    "        df_[\"Turn\"] = pred_[:,1] * w\n",
    "        df_[\"Walking\"] = pred_[:,2] * w\n",
    "        df_[\"Time\"] = time_\n",
    "        df_[\"Id\"] = id_values\n",
    "        df_[\"Id\"] = df_[\"Id\"].astype(str) + \"_\" + df_[\"Time\"].astype(str)\n",
    "        pred_list.append(df_)\n",
    "    pred = pd.concat(pred_list).reset_index(drop=True)\n",
    "    df_all.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ee2728c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-06T12:46:19.413730Z",
     "iopub.status.busy": "2023-06-06T12:46:19.413082Z",
     "iopub.status.idle": "2023-06-06T12:46:29.741647Z",
     "shell.execute_reply": "2023-06-06T12:46:29.740463Z"
    },
    "papermill": {
     "duration": 10.368803,
     "end_time": "2023-06-06T12:46:29.746950",
     "exception": false,
     "start_time": "2023-06-06T12:46:19.378147",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.91s/it]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.91s/it]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.91s/it]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.91s/it]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.91s/it]\n",
      "100%|██████████| 1/1 [00:10<00:00, 10.30s/it]\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# defog6\n",
    "# =========================\n",
    "th_len = 200000\n",
    "w = 0.10\n",
    "\n",
    "defog_list = glob.glob(DEFOG_DATA_PATH)\n",
    "cols = [\"AccV\",\"AccML\",\"AccAP\"]\n",
    "num_cols = [\"AccV\",\"AccML\",\"AccAP\",'AccV_lag_diff',\n",
    "            'AccV_lead_diff', 'AccML_lag_diff', 'AccML_lead_diff',\n",
    "            'AccAP_lag_diff', 'AccAP_lead_diff']\n",
    "for p in tqdm(defog_list):\n",
    "    id_values = p.split(\"/\")[-1].split(\".\")[0]\n",
    "    df = pd.read_csv(p)\n",
    "    if len(df) > th_len:\n",
    "        seq_len = 30000\n",
    "        shift = 15000\n",
    "        offset = 7500\n",
    "    else:\n",
    "        seq_len = 15000\n",
    "        shift = 7500\n",
    "        offset = 3750\n",
    "    batch = (len(df)-1) // shift\n",
    "    if batch == 0:\n",
    "        batch = 1\n",
    "    for c in cols:\n",
    "        df[f\"{c}_lag_diff\"] = df[c].diff()\n",
    "        df[f\"{c}_lead_diff\"] = df[c].diff(-1)\n",
    "    sc = StandardScaler()\n",
    "    df[num_cols] = sc.fit_transform(df[num_cols].values)\n",
    "    df[num_cols] = df[num_cols].fillna(0)\n",
    "    num = df[num_cols].values\n",
    "    time = df[\"Time\"].values\n",
    "    \n",
    "    num_array = np.zeros([batch,seq_len,9])\n",
    "    mask_array = np.zeros([batch,seq_len],dtype=int)\n",
    "    time_array = np.zeros([batch,seq_len],dtype=int)\n",
    "    pred_use_array = np.zeros([batch,seq_len],dtype=int)\n",
    "    \n",
    "    if len(df) <= seq_len:\n",
    "        b = 0\n",
    "        num_len = len(num)\n",
    "        num_array[b,:num_len,:] = num\n",
    "        time_array[b,:num_len] = time\n",
    "        mask_array[b,:num_len] = 1\n",
    "        pred_use_array[b,:num_len] = 1\n",
    "    else:\n",
    "        for n,b in enumerate(range(batch)):\n",
    "            if b == (batch - 1):\n",
    "                num_ = num[b*shift : ]\n",
    "                time_ = time[b*shift : ]\n",
    "                num_len = len(num_)\n",
    "\n",
    "                num_array[b,:num_len,:] = num_\n",
    "                time_array[b,:num_len] = time_\n",
    "                mask_array[b,:num_len] = 1\n",
    "                pred_use_array[b,offset:num_len] = 1\n",
    "            elif b == 0:\n",
    "                num_ = num[b*shift:b*shift+seq_len]\n",
    "                time_ = time[b*shift:b*shift + seq_len]\n",
    "\n",
    "                num_array[b,:,:] = num_\n",
    "                time_array[b,:] = time_\n",
    "                mask_array[b,:] = 1\n",
    "                pred_use_array[b,:shift+offset] = 1\n",
    "            else:\n",
    "                num_ = num[b*shift:b*shift+seq_len]\n",
    "                time_ = time[b*shift:b*shift + seq_len]\n",
    "\n",
    "                num_array[b,:,:] = num_\n",
    "                time_array[b,:] = time_\n",
    "                mask_array[b,:] = 1\n",
    "                pred_use_array[b,offset:shift+offset] = 1  \n",
    "    \n",
    "    test_ = FogDataset(num_array,\n",
    "                       mask_array,\n",
    "                       train=False)\n",
    "    test_loader = DataLoader(dataset=test_, \n",
    "                        batch_size=bs, \n",
    "                        shuffle = False)\n",
    "    for n,m in enumerate(defog_model_list6):\n",
    "        if n == 0:\n",
    "            pred = make_pred(test_loader,m) / len(defog_model_list6)\n",
    "        else:\n",
    "            pred += make_pred(test_loader,m) / len(defog_model_list6)\n",
    "    pred_list = []\n",
    "    for i in range(batch):\n",
    "        mask_ = pred_use_array[i]\n",
    "        pred_ = pred[i,mask_ == 1,:]\n",
    "        time_ = time_array[i, mask_ == 1]\n",
    "        df_ = pd.DataFrame()\n",
    "        df_[\"StartHesitation\"] = pred_[:,0] * w\n",
    "        df_[\"Turn\"] = pred_[:,1] * w\n",
    "        df_[\"Walking\"] = pred_[:,2] * w\n",
    "        df_[\"Time\"] = time_\n",
    "        df_[\"Id\"] = id_values\n",
    "        df_[\"Id\"] = df_[\"Id\"].astype(str) + \"_\" + df_[\"Time\"].astype(str)\n",
    "        pred_list.append(df_)\n",
    "    pred = pd.concat(pred_list).reset_index(drop=True)\n",
    "    df_all.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c6c40dd5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-06T12:46:29.822416Z",
     "iopub.status.busy": "2023-06-06T12:46:29.822087Z",
     "iopub.status.idle": "2023-06-06T12:46:37.150395Z",
     "shell.execute_reply": "2023-06-06T12:46:37.149096Z"
    },
    "papermill": {
     "duration": 7.368612,
     "end_time": "2023-06-06T12:46:37.152754",
     "exception": false,
     "start_time": "2023-06-06T12:46:29.784142",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.29s/it]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.34s/it]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.29s/it]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.30s/it]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.29s/it]\n",
      "100%|██████████| 1/1 [00:07<00:00,  7.29s/it]\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# defog7\n",
    "# =========================\n",
    "th_len = 200000\n",
    "w = 0.05\n",
    "\n",
    "defog_list = glob.glob(DEFOG_DATA_PATH)\n",
    "cols = [\"AccV\",\"AccML\",\"AccAP\"]\n",
    "num_cols = [\"AccV\",\"AccML\",\"AccAP\",'AccV_lag_diff',\n",
    "            'AccV_lead_diff', 'AccML_lag_diff', 'AccML_lead_diff',\n",
    "            'AccAP_lag_diff', 'AccAP_lead_diff']\n",
    "for p in tqdm(defog_list):\n",
    "    id_values = p.split(\"/\")[-1].split(\".\")[0]\n",
    "    df = pd.read_csv(p)\n",
    "    if len(df) > th_len:\n",
    "        seq_len = 30000\n",
    "        shift = 15000\n",
    "        offset = 7500\n",
    "    else:\n",
    "        seq_len = 15000\n",
    "        shift = 7500\n",
    "        offset = 3750\n",
    "    batch = (len(df)-1) // shift\n",
    "    if batch == 0:\n",
    "        batch = 1\n",
    "    for c in cols:\n",
    "        df[f\"{c}_lag_diff\"] = df[c].diff()\n",
    "        df[f\"{c}_lead_diff\"] = df[c].diff(-1)\n",
    "    sc = StandardScaler()\n",
    "    df[num_cols] = sc.fit_transform(df[num_cols].values)\n",
    "    df[num_cols] = df[num_cols].fillna(0)\n",
    "    num = df[num_cols].values\n",
    "    time = df[\"Time\"].values\n",
    "    \n",
    "    num_array = np.zeros([batch,seq_len,9])\n",
    "    mask_array = np.zeros([batch,seq_len],dtype=int)\n",
    "    time_array = np.zeros([batch,seq_len],dtype=int)\n",
    "    pred_use_array = np.zeros([batch,seq_len],dtype=int)\n",
    "    \n",
    "    if len(df) <= seq_len:\n",
    "        b = 0\n",
    "        num_len = len(num)\n",
    "        num_array[b,:num_len,:] = num\n",
    "        time_array[b,:num_len] = time\n",
    "        mask_array[b,:num_len] = 1\n",
    "        pred_use_array[b,:num_len] = 1\n",
    "    else:\n",
    "        for n,b in enumerate(range(batch)):\n",
    "            if b == (batch - 1):\n",
    "                num_ = num[b*shift : ]\n",
    "                time_ = time[b*shift : ]\n",
    "                num_len = len(num_)\n",
    "\n",
    "                num_array[b,:num_len,:] = num_\n",
    "                time_array[b,:num_len] = time_\n",
    "                mask_array[b,:num_len] = 1\n",
    "                pred_use_array[b,offset:num_len] = 1\n",
    "            elif b == 0:\n",
    "                num_ = num[b*shift:b*shift+seq_len]\n",
    "                time_ = time[b*shift:b*shift + seq_len]\n",
    "\n",
    "                num_array[b,:,:] = num_\n",
    "                time_array[b,:] = time_\n",
    "                mask_array[b,:] = 1\n",
    "                pred_use_array[b,:shift+offset] = 1\n",
    "            else:\n",
    "                num_ = num[b*shift:b*shift+seq_len]\n",
    "                time_ = time[b*shift:b*shift + seq_len]\n",
    "\n",
    "                num_array[b,:,:] = num_\n",
    "                time_array[b,:] = time_\n",
    "                mask_array[b,:] = 1\n",
    "                pred_use_array[b,offset:shift+offset] = 1  \n",
    "    \n",
    "    test_ = FogDataset(num_array,\n",
    "                       mask_array,\n",
    "                       train=False)\n",
    "    test_loader = DataLoader(dataset=test_, \n",
    "                        batch_size=bs, \n",
    "                        shuffle = False)\n",
    "    for n,m in enumerate(defog_model_list7):\n",
    "        if n == 0:\n",
    "            pred = make_pred(test_loader,m) / len(defog_model_list7)\n",
    "        else:\n",
    "            pred += make_pred(test_loader,m) / len(defog_model_list7)\n",
    "    pred_list = []\n",
    "    for i in range(batch):\n",
    "        mask_ = pred_use_array[i]\n",
    "        pred_ = pred[i,mask_ == 1,:]\n",
    "        time_ = time_array[i, mask_ == 1]\n",
    "        df_ = pd.DataFrame()\n",
    "        df_[\"StartHesitation\"] = pred_[:,0] * w\n",
    "        df_[\"Turn\"] = pred_[:,1] * w\n",
    "        df_[\"Walking\"] = pred_[:,2] * w\n",
    "        df_[\"Time\"] = time_\n",
    "        df_[\"Id\"] = id_values\n",
    "        df_[\"Id\"] = df_[\"Id\"].astype(str) + \"_\" + df_[\"Time\"].astype(str)\n",
    "        pred_list.append(df_)\n",
    "    pred = pd.concat(pred_list).reset_index(drop=True)\n",
    "    df_all.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "01dbf441",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-06T12:46:37.237151Z",
     "iopub.status.busy": "2023-06-06T12:46:37.235251Z",
     "iopub.status.idle": "2023-06-06T12:46:39.400173Z",
     "shell.execute_reply": "2023-06-06T12:46:39.399054Z"
    },
    "papermill": {
     "duration": 2.209615,
     "end_time": "2023-06-06T12:46:39.403138",
     "exception": false,
     "start_time": "2023-06-06T12:46:37.193523",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_all = pd.concat(df_all).reset_index(drop=True)\n",
    "df_all = df_all.groupby(by=\"Id\")[['StartHesitation', 'Turn', 'Walking']].sum().reset_index()\n",
    "df_all[['Id', 'StartHesitation', 'Turn', 'Walking']].to_csv(\"submission.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7d422f8b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-06T12:46:39.488583Z",
     "iopub.status.busy": "2023-06-06T12:46:39.488204Z",
     "iopub.status.idle": "2023-06-06T12:46:39.507620Z",
     "shell.execute_reply": "2023-06-06T12:46:39.506574Z"
    },
    "papermill": {
     "duration": 0.065236,
     "end_time": "2023-06-06T12:46:39.509906",
     "exception": false,
     "start_time": "2023-06-06T12:46:39.444670",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>StartHesitation</th>\n",
       "      <th>Turn</th>\n",
       "      <th>Walking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>003f117e14_0</td>\n",
       "      <td>0.002737</td>\n",
       "      <td>0.003436</td>\n",
       "      <td>0.000623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>003f117e14_1</td>\n",
       "      <td>0.002624</td>\n",
       "      <td>0.003394</td>\n",
       "      <td>0.000576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>003f117e14_10</td>\n",
       "      <td>0.002088</td>\n",
       "      <td>0.002961</td>\n",
       "      <td>0.000458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>003f117e14_100</td>\n",
       "      <td>0.001648</td>\n",
       "      <td>0.002645</td>\n",
       "      <td>0.000372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>003f117e14_1000</td>\n",
       "      <td>0.002842</td>\n",
       "      <td>0.037812</td>\n",
       "      <td>0.000685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286365</th>\n",
       "      <td>02ab235146_99995</td>\n",
       "      <td>0.000296</td>\n",
       "      <td>0.277196</td>\n",
       "      <td>0.036089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286366</th>\n",
       "      <td>02ab235146_99996</td>\n",
       "      <td>0.000290</td>\n",
       "      <td>0.275381</td>\n",
       "      <td>0.035312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286367</th>\n",
       "      <td>02ab235146_99997</td>\n",
       "      <td>0.000286</td>\n",
       "      <td>0.275587</td>\n",
       "      <td>0.034789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286368</th>\n",
       "      <td>02ab235146_99998</td>\n",
       "      <td>0.000284</td>\n",
       "      <td>0.278020</td>\n",
       "      <td>0.034215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286369</th>\n",
       "      <td>02ab235146_99999</td>\n",
       "      <td>0.000281</td>\n",
       "      <td>0.281292</td>\n",
       "      <td>0.033943</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>286370 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Id  StartHesitation      Turn   Walking\n",
       "0           003f117e14_0         0.002737  0.003436  0.000623\n",
       "1           003f117e14_1         0.002624  0.003394  0.000576\n",
       "2          003f117e14_10         0.002088  0.002961  0.000458\n",
       "3         003f117e14_100         0.001648  0.002645  0.000372\n",
       "4        003f117e14_1000         0.002842  0.037812  0.000685\n",
       "...                  ...              ...       ...       ...\n",
       "286365  02ab235146_99995         0.000296  0.277196  0.036089\n",
       "286366  02ab235146_99996         0.000290  0.275381  0.035312\n",
       "286367  02ab235146_99997         0.000286  0.275587  0.034789\n",
       "286368  02ab235146_99998         0.000284  0.278020  0.034215\n",
       "286369  02ab235146_99999         0.000281  0.281292  0.033943\n",
       "\n",
       "[286370 rows x 4 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 71.84614,
   "end_time": "2023-06-06T12:46:40.974462",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-06-06T12:45:29.128322",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
